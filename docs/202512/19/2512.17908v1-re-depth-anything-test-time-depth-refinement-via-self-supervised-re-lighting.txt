Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised
Re-lighting
Ananta R. Bhattarai
Bielefeld University
Helge Rhodin
Bielefeld University
Input Image
DA-V2
Ours
Frontal
Left Novel Views
Right Novel Views
Test-time
Similar Real
Tiger Image
Figure 1. Re-Depth Anything refines the prediction of Depth Anything V2 [53] by re-lighting the reconstructed geometry and extracting
knowledge from diffusion models in a self-supervised manner. In this example, the test-time optimization enhances facial detail (see frontal
view) and refines the nose shape to look more like a tiger (side view), correcting the dog-like initial resemblance likely originating from a
biased training distribution. The key contribution is a re-synthesis method that replaces photometric reconstruction for self-supervision.
Abstract
Monocular depth estimation remains challenging as recent
foundation models, such as Depth Anything V2 (DA-V2),
struggle with real-world images that are far from the train-
ing distribution. We introduce Re-Depth Anything, a test-
time self-supervision framework that bridges this domain
gap by fusing DA-V2 with the powerful priors of large-scale
2D diffusion models. Our method performs label-free re-
finement directly on the input image by re-lighting predicted
depth maps and augmenting the input. This re-synthesis
method replaces classical photometric reconstruction by
leveraging shape from shading (SfS) cues in a new, genera-
tive context with Score Distillation Sampling (SDS). To pre-
vent optimization collapse, our framework employs a tar-
geted optimization strategy: rather than optimizing depth
directly or fine-tuning the full model, we freeze the encoder
and only update intermediate embeddings while also fine-
tuning the decoder. Across diverse benchmarks, Re-Depth
Anything yields substantial gains in depth accuracy and re-
alism over the DA-V2, showcasing new avenues for self-
supervision by augmenting geometric reasoning.
1. Introduction
Monocular Depth Estimation (MDE) aims to predict dense,
per-pixel depth from a single RGB image, enabling numer-
ous applications, including 3D reconstruction [13, 26, 55],
autonomous driving [46], robotic navigation [50], and vir-
tual or augmented reality [34]. Lately, high-quality depth
maps for diverse images were enabled by foundation mod-
els using Vision Transformers (ViTs) [7] with dense pre-
diction heads [33]. Crucially, MiDaS [32] pioneered the
training on a myriad of labeled datasets and Depth Any-
thing V2 (DA-V2) [53] showed that the sparse and often
noisy depth labels can be enhanced with a teacher model
trained on synthetic data. While these foundational models
are setting new state-of-the-art performance, inaccuracies
on in-the-wild reconstruction remain (see Fig. 1). Monoc-
ular depth estimation remains one of the fundamental yet
challenging problems in computer vision.
In this work, we introduce Re-Depth Anything, a test-
time optimization framework designed to close the domain
gap for DA-V2 by self-supervision through 2D generative
models. Given a single input image, our framework adapts
the pre-trained DA-V2 model to the specific image content.
1
arXiv:2512.17908v1  [cs.CV]  19 Dec 2025


The core idea is to re-light the DA-V2 depth map in random
illumination conditions and to superimpose these onto the
input image. The depth map is then refined by using a 2D
diffusion model as a prior for scoring how realistic the aug-
mented shading is. This plausibility estimate is backprop-
agated to the depth map through a differentiable renderer
using the Blinn-Phong illumination model [3] and the SDS
loss. Crucially, instead of directly optimizing depth or fine-
tuning the entire network, we propose a targeted optimiza-
tion strategy to jointly optimize only the intermediate fea-
ture embeddings fed to the Dense Prediction Transformer
(DPT) decoder and the decoder‚Äôs weights. This targeted ap-
proach preserves the strong geometric knowledge encoded
in the embeddings while refining the final output.
3D knowledge distillation from 2D image diffusion mod-
els [30, 36, 37] using the SDS loss and shading cues was
pioneered by DreamFusion [31] for text-to-3D generation.
Their key ingredient is to optimize a 3D NeRF repre-
sentation [26], such that its 2D renderings are perceived
as realistic by the diffusion model. This and subsequent
works [20, 22, 43, 47, 63] have enabled reconstruction of
real images by pairing virtual views with the photometric
reconstruction of the real one. However, this line of purely
self-supervised learning from geometric relations suffers
from cue ambiguities and lags behind supervised models.
Our key advance is to apply the benefits of self-supervised
learning on top of supervised methods by re-lighting the
predicted depth map. This re-synthesizing and augment-
ing of the input image is fundamentally different to the
photometric reconstruction with a full-fledged NeRF [26]
or Gaussian Splatting [13] renderer in prior self-supervised
work and alleviates the problem of reconstructing appear-
ance pixel-perfect. Our main contributions are:
‚Ä¢ We propose Re-Depth Anything, a novel test-time opti-
mization framework that adapts pre-trained DA-V2 to real
world images using a 2D diffusion prior on re-synthesized
depth predictions, requiring no additional labeled data.
‚Ä¢ We propose a single-image re-lighting model that differ-
entiably links the predicted depth map to the input image,
enabling the use of an SDS loss for self-supervised geom-
etry refinement from a single view.
‚Ä¢ We introduce a targeted optimization scheme that
jointly optimizes the decoder‚Äôs input embeddings and its
weights, which we show is crucial for avoiding overfitting
and preserving geometric structure.
2. Related Work
Our work, Re-Depth Anything, builds upon progress in
three primary research areas: monocular depth estimation,
test-time adaptation for the monocular depth estimation
task, and the use of 2D diffusion models as priors for 3D
reconstruction. Below, we review the most related methods
from each of these domains.
Monocular Depth Estimation.
Monocular depth estima-
tion has been a long-standing challenge in computer vision.
Early approaches [1, 8, 15, 18, 40, 51, 54, 56, 58] relied
heavily on supervised learning, training on datasets with
ground-truth depth, such as KITTI [9] for outdoor driving
and NYU Depth V2 [41] for indoor scenes. More recently,
the field has shifted towards building general-purpose foun-
dation models for depth. MiDaS [32] enabled joint training
on multiple datasets by predicting disparity instead of depth
and by normalizing predictions to unit range, enabling zero-
shot generalization to new domains. Even better perfor-
mance is possible with DPT heads [33] and large diffusion
models [12, 61]. DA [52] and its successor, DA-V2 [53],
use the same relative prediction and gained further improve-
ments by training on massive-scale datasets of images by
aligning the predictions of a teacher model to sparse ground
truth measurements. It mitigates but does not resolve the
noise in the LiDAR-based ground truth on in-the-wild im-
ages.
Another line of research predicts absolute depth with-
out the disparity normalization, from single [4, 29, 44, 57]
or multiple images [5, 14, 45]. We focus on relative depth
prediction as these models typically lead to higher surface
detail, which we aim to improve. Moreover, the absolute
depth scale is invariant to shading cues. Hence, we use the
recent and popular DA-V2 model as our foundation, aim-
ing to correct its errors rather than retraining it, such that it
applies to underrepresented and out-of-distribution inputs.
Test-Time Adaptation for MDE.
Test-Time Adaptation
(TTA) or Test-Time Optimization (TTO) aims to adapt a
pre-trained model to a specific test input at inference time.
In the context of MDE, TTA often relies on self-
supervision signals available from the input itself.
For
video inputs, temporal and photometric consistency be-
tween frames is a powerful signal used to fine-tune a depth
network [16, 19, 28].
However, these methods are not applicable to the single-
image setting, which is more challenging due to the scarcity
of self-supervision cues. Some single-image TTA methods
adapt a relative depth model to predict metric depth using
strong external priors like 3D human meshes [62] or using
sparse 3D points from an external source [23]. In contrast,
our method requires no such specific external data, and in-
stead leverages a general-purpose 2D diffusion model to re-
fine relative depth for any arbitrary scene. Crucially, instead
of relying solely on the input image‚Äôs internal consistency,
we introduce a powerful prior to provide a dense, geometry-
aware supervisory signal for adaptation.
2D Diffusion Models as Priors for 3D
2D image diffu-
sion models [30, 36, 37], trained on internet-scale image
and text data, have learned incredibly rich priors about the
2


visual world. A recent line of work has focused on lever-
aging these 2D priors for 3D tasks.
The most influen-
tial is DreamFusion [31], which introduced the SDS loss,
enabling the use of a pre-trained text-to-image diffusion
model as a loss function to optimize a 3D NeRF repre-
sentation from scratch using only a text prompt. This con-
cept has been extended and improved by numerous follow-
ups [22, 25, 43, 43, 63], including using mesh representa-
tions [20, 47] and Gaussian Splatting [6, 42] as differen-
tiable renderers. Reconstructing a real input image brings
additional challenges. RealFusion [24] proposes a method
to combine real and synthetic views through a diffusion
model fine-tuning and carefully selecting compatible virtual
views while others directly fine-tune a 2D diffusion model
for multi-view reconstruction [21]. Our re-lighting princi-
ple is inspired by the DreamTexture [2] variant, which uti-
lizes shape from texture cues through virtual augmentation.
However, all of these fully unsupervised methods have not
demonstrated advantages over the latest supervised depth
estimation techniques.
Our work builds directly on this idea but applies it in a
novel context. Instead of generating a 3D shape or optimiz-
ing a NeRF on multiple views, we use the SDS loss on a
single image to refine the parameters of a pre-trained, feed-
forward depth estimation model (DA-V2) using re-lighting
as opposed to photometric reconstruction. This test-time
optimization adapts the model‚Äôs prediction to the specific
image content, guided by the diffusion model‚Äôs knowledge
about the shading of natural objects.
Single-View
Geometry
and
Shading.
Shape-from-
Shading (SfS) [10, 60] is a classical attempt to recover 3D
shape from shading variations in a single 2D image. The
idea is to decompose the image into (piecewise-constant)
albedo and shading components, e.g. using the diffuse
and specular components of a Blinn-Phong [3] model.
However, classical SfS [10, 60], and other shape from
X methods, such as Shape-from-Texture (SfT) [48, 49]
are highly ill-posed, relying on strong assumptions about
lighting, texture regularity, and material properties, which
are rarely met in practice.
DreamFusion and RealFusion were the first to revisit the
SfS principle in a modern context using generative mod-
els. Although they could lift some assumptions, RealFu-
sion still follows the classical reconstruction principle of
rendering a synthetic object that is optimized to reconstruct
the input image. However, real illumination and material
properties are complex, leading to artifacts around specular
highlights when attempting to match them with simplistic
shading models in the deployed differentiable renderers.
By contrast, we do not attempt to solve the full, ill-posed
inverse graphics problem. Instead, one of our key contribu-
tions is to augment with additional shading cues, which is
achieved with a modification of a simple, lightweight Blinn-
Phong [3] renderer. This allows the diffusion prior to cri-
tique the plausibility of the 3D shape as expressed through
its shading, effectively linking the underlying geometry and
image content without photometric reconstruction.
3. Preliminaries and Notation
Score Distillation Sampling
Given a 2D image X, ren-
dered from a differentiable representation with parameters
Œ∏, SDS [31] utilizes a pre-trained diffusion model œï to opti-
mize Œ∏ via gradient descent. Specifically, a gradient toward
a more likely image is obtained from the noise œµœï predicted
by œï given a noisy image Xt, text embedding c, and the
noise level t,
  \label {e q :
sds
}
 \na bla _{\ th et a  }  \
ma
t
hcal {L}_{\text {SDS}}(\mathbf {X}, c) = \mathop {\mathbb {E}}_{\epsilon , t} \left [ w(t) \left ( \epsilon _{\phi }(\mathbf {X}_t; c, t) - \epsilon \right ) \frac {\partial \mathbf {X}}{\partial \theta } \right ], 
(1)
where w(t) is a weighting function and œµ ‚àºN(0, I). In
DreamFusion [31], the SDS loss matches the 2D renderings
from random angles to the text prompt. In our work, we
adapt apply the SDS loss on the re-lit image and adapt it
to optimize both the intermediate feature embeddings and
decoder weights in DA-V2 to enhance depth prediction.
Depth Anything V2 Architecture
DA-V2 [53] follows
the recent trend of using ViT encoders as in DINOv2 [27]
and DPT [33] for the disparity decoder. Specifically, the
encoder transforms input tokens into new feature represen-
tations using L transformer layers. Features from four se-
lected layers are extracted and passed to the DPT head for
disparity prediction, with layer selection depending on the
ViT variant. For example, layers l = {3, 6, 9, 12} are used
in the ViT-Small configuration. Given an input image I, we
denote the extracted feature representations as embeddings
W. The final disparity ÀÜDdisp = f(W; Œ∏) is predicted by the
DPT head f(¬∑), parameterized by pre-trained weights Œ∏ and
taking embeddings W as input.
4. Method
Given an input image I ‚ààRC√óH√óW , our goal is to re-
fine an initial disparity map estimate ÀÜDinit ‚ààRH√óW pre-
dicted by the pre-trained DA-V2 [53] model. As illustrated
in Fig. 2, Re-Depth Anything, is a self-supervised, test-time
optimization framework that adapts the DA-V2 model to the
specific input image.
Key to our method is how we use the SDS [31] loss as a
2D diffusion prior for 3D refinement. To this end, we first
introduce a differentiable rendering function that links the
predicted disparity map ÀÜDdisp to a re-illuminated image ÀÜI
through augmentation. We then use the SDS loss on ÀÜI to
jointly optimize the decoder‚Äôs input embeddings W and the
3


Depth Anything V2
Transformer
Encoder
DPT
Head
Embeddings
Re-lighting
Input Image
Disparity Map
Normal from 
Disparity Map
Rendering
Diffuse 
Shading
Normal
Rand 
Light
Specular 
Shading
Input Image
Shaded Image
Shaded Image
Rendering
‚äô
+
=
ùõΩ1
‚äô
ùõΩ2
‚äô
Stable
Diffusion
SDS Loss
Backpropagate onto embeddings and DPT weights
BLIP-2
‚ÄúA high-quality photo of a tiger‚Äù
Figure 2. Re-Depth Anything overview. Our main contribution is the re-lighting module that randomizes light conditions and shades
the estimated geometry on the input. Notably, the re-lighting does not need to look physically accurate as we are only augmenting not
photometrically reconstructing the image. Key is also the SDS optimization of embeddings and decoder, while leaving the encoder frozen.
weights Œ∏ to better align re-illuminated and original image.
We describe each component in detail below.
4.1. Shaded Depth Rendering
To leverage a 2D diffusion prior, we must establish a dif-
ferentiable link between the depth map ÀÜD and a 2D image.
A full inverse rendering approach, which would decompose
the scene into albedo, lighting, materials, and 3D geometry,
is highly ill-posed from a single image.
Instead, we propose to augment the input image with ad-
ditional shading effects by re-lighting. We synthesize dif-
fuse and specular reflectance maps with the classical Blinn-
Phong shading model [3], as a function of the normals N of
the depth map. Computing the normals at pixel coordinates
u, v requires a camera model. We test a scaled orthographic
and a perspective projection with intrinsic matrices Korth
and Kpersp, respectively. We then unproject the depth map
element-wise into a 3D mesh with vertices
  \mX
 = \m
K
 
_ \t
e xt
 {
p
e rs p } ^{-
1} \
b
e
g
i
n 
{
p matrix} \mU \mathbf {\hat {D}}\\ \mV \mathbf {\hat {D}}\\ \mathbf {\hat {D}}\\ \end {pmatrix} \text { or } \mX = \mK _\text {orth}^{-1} \begin {pmatrix} \mU \\ \mV \\ \mathbf {\hat {D}}\\ \end {pmatrix} , \label {eq:unprojection} 
(2)
with U and V the tensor of horizontal and vertical pixel co-
ordinate ranging from ‚àí1 to 1. The normal N is orthogonal
to the spatial gradients (‚àáXu, ‚àáXv), computed across the
entire image using the element-wise cross product,
  
\mN  = \
frac { \nabla
 \mX _v \times \nabla \mX _u}{||\nabla \mX _v \times \nabla \mX _u||_2}. 
(3)
Crucial for our re-lighting of the input image is to use
the inverse-tonemapped input image, œÑ ‚àí1(I), as a proxy
for the scene‚Äôs diffuse albedo and we assume that specu-
lar highlights are colorless, not affected by albedo. While
not physically accurate, it exploits that illumination effects
are linear and there remains ambiguity between light and
surface color, providing a sufficient, differentiable connec-
tion between geometry and re-lit appearance. Specifically,
we synthesize a re-illuminated image ÀÜI ‚ààRC√óH√óW using
Blinn-Phong shading [3],
  \ l
a
be l {eq:re nder ing} \ma thbf {\h at {
I}} = \tau \big (\beta _1 \max (\mathbf {N} \cdot \mathbf {l}, 0) \odot \tau ^{-1}(\mathbf {I}) + \beta _2 \max (\mathbf {N} \cdot \mathbf {h}, 0)^{\alpha }\big ), (4)
where N ‚ààR3√óH√óW is the per-pixel normal map derived
from the depth gradients of ÀÜD, l ‚ààR3 is the light direction,
h ‚ààR3 is the halfway vector between l and the view direc-
tion v = [0, 0, 1]T , and Œ±, Œ≤1, and Œ≤2 are material and light
intensity parameters that are determined in the next section.
The tone-mapping function œÑ(I) = I1/Œ≥, with Œ≥ = 2.2,
ensures that shading is performed in the linear RGB color
space.
Note that using I as the albedo can double the shad-
ing effects (e.g. existing shadows get darker) and the addi-
tion of specular maps may saturate the image, which, how-
ever, rarely in our experiments and is similar to specular
highlights appearing white in photographs. We keep val-
ues within bounds by clamping the rendered output ÀÜI to the
range [10‚àí3, 1].
Handling normalized relative depth.
DA-V2 outputs
normalized relative depths in the form of the normalized
disparity map ÀÜDdisp = (1/ ÀÜD ‚àím)s, where m is the min-
imum disparity and s is one over the maximum‚Äìminimum
range of disparity. Converting to absolute depth requires
  \
l
abel {eq : d
i
s
parity t od
epth} \mathbf {\hat {D}} = \frac {1}{\mathbf {\hat {D}}_\text {disp}/s + m} = \frac {s}{\mathbf {\hat {D}}_\text {disp} + m s}, 
(5)
where neither the scaling s nor the offset m is known at test
time. However, the normal is invariant to the global scale
and hence we only have to optimize the unknown scalar
b = ms alongside the depth refinement. Notably, we found
the optimization to be insensitive to these parameters, likely
because shading is about the relative angle between the light
and the normal, not absolute orientation. Specifically, it is
sufficient to fix b = 0.1 with scaled orthographic projection.
4


4.2. Augmentation Objective
Our goal is to refine the depth map to yield shading effects
that yield plausible re-lightings of the input image. This
augmentation principle lets us choose random light and ma-
terial properties instead of having to estimate parameters for
the potentially absent or complex shading effects in the in-
put image when doing photometric reconstruction. At each
optimization step, we randomly sample the light direction l
and diffuse and specular intensities (Œ≤1, Œ≤2) and exponent Œ±
to ensure the refined geometry is consistent with the image
across diverse shading conditions.
Loss Function.
Plausibility of the augmentation is mea-
sured by the total loss combining the generative prior with
a smoothness regularizer,
  \la be l {eq:l o ss} \mat hc a l 
{L
}
(\m
at hbf {
\hat {I}}, c, \mathbf {\hat {D}}_\text {disp}) = \mathcal {L}_{\text {SDS}}(\mathbf {\hat {I}}, c) + \frac {\lambda _1}{hw} \sum _{i,j} \| \Delta \mathbf {\hat {D}}^{i,j}_{\text {disp}} \|^2, 
(6)
where LSDS is the SDS loss defined in Eq. (1), and ÀÜI is the
image rendered from ÀÜD using Eq. (4). The second term is an
L1 regularizer on the disparity gradients, which encourages
smoother surfaces and prevents noisy artifacts.
To obtain the conditioning text prompt c, we employ
BLIP-2 [17], a state-of-the-art image-to-text model, to gen-
erate a descriptive caption for the input image I.
4.3. Optimization Scheme
Our goal is to refine the output depth map of feed-forward
depth estimators, specifically the DA-V2 model.
How-
ever, directly optimizing the depth map on the proposed
re-lighting loss remains an ambigious problem with many
plausible solutions. Instead, we propose optimizing the la-
tent feature space and weights of DA-V2, thereby leverag-
ing the prior on 3D shapes learned at training time.
Candidates for optimization are the entire DA-V2
weights or its components. We found that fine-tuning the
entire DA-V2 tends to fall into poor local minima or cause
the geometry to overfit to image textures. To address this,
we jointly optimize only the intermediate embeddings W
(the intermediate feature embeddings of the frozen ViT en-
coder) and the DPT head‚Äôs weights Œ∏.
  \ ma t hbf  {W
}^* , \th et a ^* = \arg \min _{\mathbf {W}, \theta } \mathcal {L}(\mathbf {\hat {I}}, c, \mathbf {\hat {D}}_\text {disp}) 
(7)
where ÀÜDdisp = f(W; Œ∏) and ÀÜI is a function of ÀÜDdisp.
Depth Map Ensembling.
The stochastic nature of the
SDS loss, primarily due to the random sampling of noise
œµ and timestep t, can lead to high variance in the optimiza-
tion results. Consequently, disparity predictions can vary
noticeably across different runs.
To stabilize the final prediction, inspired by the ensem-
bling in Marigold [12], we perform the optimization N
times with a different random seed. We then aggregate the
resulting disparity maps by a simple mean operation. The
final disparity map Ddisp is obtained as
  \mD  _
{
\
t
ext
 {di
s p }}
 = \frac {1}{N}\sum _{i=1}^{N} f(\mathbf {W}_i^*; \theta _i^*), 
(8)
where (W‚àó
i , Œ∏‚àó
i ) are the optimized embeddings and decoder
weights from the i-th run.
5. Experiments
We study the effectiveness and accuracy of our self-
supervised re-lighting approach on three benchmarks,
demonstrating consistent improvements over the DA-V2
baseline across the established metrics, including a relative
improvement up to 12.6%. Fig. 3 shows exemplary cases
that are improved by removing noise from flat areas and
adding missing details. Additional results are shown in the
supplemental document.
Baselines.
We utilize the small variant of the DA-V2 ar-
chitecture as our base model, which is one of the most pop-
ular monocular relative depth estimation methods. Besides
various baseline variants using only parts of our contribu-
tions, we also compare against a simple Shape from Shad-
ing implementation, to demonstrate the advantage of re-
lighting rather than photometric reconstruction.
Implementation Details.
For all experiments, input im-
ages are first resized, maintaining their original aspect ratio,
such that at least one side measures 518 pixels, to match the
training resolution of the DA-V2 model. Before applying
Stable Diffusion, the images are further zero-padded if they
have a non-square aspect ratio and resized to 512 √ó 512.
Our entire pipeline is implemented in PyTorch. For SDS
guidance, we employ v1.5 of Stable Diffusion [36]. We op-
timize the encoder‚Äôs embeddings and DPT weights for 1000
iterations using the AdamW optimizer. We set a learning
rate of 1 √ó 10‚àí3 for the embeddings and 2 √ó 10‚àí6 for the
DPT weights. We set the regularization weight Œª1 to 1.0.
At each optimization step, we uniformly sample two coeffi-
cients (Œ≤1, Œ≤2) ‚àºU[0, 1] and an exponent Œ±, where Œ± = 2k
and k ‚àºU[2, 8]. Œ≤1 and Œ≤2 are subsequently normalized
to ensure their sum is 1.0 (i.e. Œ≤1 + Œ≤2 = 1). Similarly,
the light direction vector l = (Lx, Ly, Lz) is sampled by
drawing the X and Y coordinates from a uniform distribu-
tion, Lx, Ly ‚àºU[‚àí1, 1], while fixing the Z coordinate to
Lz = 1. The resulting vector l is then L2-normalized. In
the absence of known camera parameters on in-the-wild im-
ages, our default is a scaled orthographic camera and we
5


Table 1. Comparison with DA-V2 across datasets. Relative error reduction of Ours over DA-V2 is shown in the last row of each dataset.
Dataset
Method
Higher is better ‚Üë
Lower is better ‚Üì
Œ¥1
Œ¥2
Œ¥3
AbsRel
RMSE
log10
RMSE log
SI log
SqRel
CO3D
DA-V2
1.0
1.0
1.0
0.00227
0.0602
0.000985
0.00321
0.321
0.000244
Ours
1.0
1.0
1.0
0.00223
0.0588
0.000968
0.00314
0.314
0.000235
Rel. ‚àÜ(%)
-
-
-
1.75
2.26
1.74
2.24
2.24
3.66
KITTI
DA-V2
0.568
0.796
0.902
0.305
7.01
0.118
0.348
33.6
2.49
Ours
0.593
0.818
0.917
0.283
6.71
0.110
0.319
30.7
2.20
Rel. ‚àÜ(%)
5.73
10.9
15.3
7.10
4.29
6.55
8.51
8.51
11.4
ETH3D
DA-V2
0.884
0.956
0.978
0.113
0.955
0.0448
0.153
15.1
0.391
Ours
0.898
0.965
0.982
0.104
0.875
0.0413
0.143
14.1
0.347
Rel. ‚àÜ(%)
12.2
21.1
19.5
8.30
8.39
7.72
6.44
6.22
11.1
optimize the scaling starting from seven. For perspective
camera, the focal length is initialized to two and refined
alongside the disparity optimization. The final prediction is
generated by aggregating the results from 10 optimization
runs. Each of these 10 runs is initialized from the original
pre-trained weights of the DA-V2 model. We conduct our
evaluation at the resolution of the initial 518-pixel-side re-
sized input image. One run takes approximately 80 seconds
on a single NVIDIA RTX 5000.
Datasets.
We evaluate Re-Depth Anything on three stan-
dard benchmarking datasets: CO3Dv2 [35] contains multi-
view images of several objects across 50 categories, with
camera poses, 3D point clouds, foreground masks, and
depth maps that we utilize for sparse depth ground truth.
From each sequence with a valid depth map, we randomly
selected two images. This pre-processing step yielded a to-
tal of 80 images from 20 object categories.
KITTI [9] is a large-scale autonomous driving dataset
featuring sparse metric depth captured by a LiDAR sensor.
We randomly sampled 10 images from each sequence of the
official validation set, resulting in a total of 130 images.
ETH3D [39] is a high-resolution benchmark with both
indoor and outdoor scenes. We randomly sampled ten raw
images and their corresponding depth maps from 13 scenes.
This process resulted in a total of 130 images for evaluation.
Evaluation protocol.
We apply the widely adopted met-
rics for assessing the quality of monocular depth estimation:
Œ¥1, Œ¥2, Œ¥3, AbsRel, RMSE, log10, RMSE log, SI log and
SqRel. We compute these metrics on absolute depth maps,
obtained by first finding the least-squares affine fit to the la-
bel in disparity, then converting to depth, and subsequently
finding the affine fit in depth, as is typical on these bench-
marks for relative depth prediction [53].
5.1. Quantitative Evaluation
Table 1 shows that our test-time refinement improves on
DA-V2 across CO3D, KITTI, and ETH3D on all nine eval-
uation metrics. Notably, we achieve significant relative re-
ductions in error, including 8.5% in SI log and RMSE log
on KITTI, alongside an 8.4% in AbsRel on ETH3D.
On CO3D, the delta metrics are saturated and errors are
smaller overall, leading to smaller yet still consistent im-
provements. This robustness over nine different metrics val-
idates the efficacy of our self-supervised approach, demon-
strating considerable potential for fine-tuning foundational
models.
Notably, the improvements are consistent across CO3D,
which covers single objects in a close-up setting, to street
scenes in the KITTI dataset, and indoor scenes in the
ETH3D dataset. This highlights the strong generalization
capability of our test-time re-lighting approach, inherited
from the robustness of generative image models. The na-
ture of the improvement is best explained at examples.
5.2. Qualitative Evaluation
For visual assessment, we present a qualitative comparison
against DA-V2 in Fig. 3. Re-Depth Anything produces vis-
ibly superior results by enhancing fine-grained details such
as the threads on a ball (first image), balcony railings, and
electricity wires (second-to-last image), but also removing
noise from flat surfaces, as indicated by an arrow in the
fourth example. These qualitative improvements are con-
sistent with the quantitative gains reported in Table 1.
We also compare against classical shape from shading,
which, however, fails drastically if its strict assumptions are
violated. For instance, even on the relatively simple ball
example in Fig. 4 (first row), the discoloring of the leather
leads to spurious and noisy normals. This highlights the
importance of our re-lighting augmentation strategy, which
does not make an assumption on albedo constancy and does
not suffer from the seam artifacts typically associated with
shape from shading.
6


Input Image
GT
DA-V2
Ours
DA-V2
Ours
Disparity Maps
Surface Normals
CO3D
ETH3D
KITTI
Figure 3. Qualitative Comparison, highlighting the added detail (rows 1,2,3,6) and noise-removal effects (rows 4,5).
7


Input
Image
GT
Disparity
Pred.
Disparity
Pred.
Normal
SfS
Tensor
Opt.
Full
Opt.
Ours
Abs Rel: 0.000881 
Abs Rel: 00201 
Abs Rel: 0.000566 
Figure 4. Qualitative ablation showing that optimizing depth di-
rectly or fine-tuning the whole network at once are detrimental.
The listed error values relate to visual and quantitative improve-
ments.
Limitations.
We rarely observed small hallucinated edges
as for the sticker on the truck in Fig. 3, which is plausi-
ble but incorrect. Sometimes the method extends geome-
try into the sky and over-smooths fine details, such as trees
Input 
Image
DA-V2 
Normal
Ours
Figure 5. Limitations.
in dark regions, as seen on
the KITTI example (see in-
set Fig. 5), which could
potentially be handled by
thresholding.
In general,
we found single objects
in the CO3D dataset show
stronger
detail
improve-
ments, while in room and
street scenes the largest
gains are from removing
suspicious details in the initial DA-V2 predictions, which
leads to unrealistic re-lighting highlights, and are hence ef-
fectively removed by our method while preserving actual
detail.
5.3. Ablation study
Optimization
We present an ablation study on the CO3D
evaluation set, comparing our full pipeline to a baseline
lacking the SDS loss, and four optimization variants: (1)
direct depth pixel optimization, (2) full DA-V2 fine-tuning,
(3) fine-tuning only embeddings and DPT weights, and (4) a
two-stage version of (3) that first optimizes the embeddings
Table 2. Ablation on the CO3D dataset showing the significance
of the major design choices.
Method
AbsRel ‚Üì
RMSE ‚Üì
log10 ‚Üì
RMSE log ‚Üì
SI log ‚Üì
SqRel ‚Üì
w/o LSDS
0.00427
0.0993
0.00185
0.00532
0.532
0.000661
Tensor Model
0.00226
0.0601
0.000985
0.00321
0.321
0.000244
Full Model
0.00331
0.0779
0.00143
0.00418
0.418
0.000412
Two Stage
0.00225
0.0597
0.000979
0.00319
0.319
0.000241
Ours
0.00223
0.0588
0.000968
0.00314
0.314
0.000235
Figure 6. Ensemble size vs. improvement in SI log on CO3D.
and subsequently fine-tunes the decoder.
The qualitative results in Fig. 4 show that direct opti-
mization (1) creates noise artifacts (first row), while full
finetuning (2) causes the geometry to collapse (second row),
even-though we reduced to a very small learning rate of
2 √ó 10‚àí6 for optimizing both the ViT encoder and the DPT
decoder.
Our chosen approach (3) strikes a balance, enhancing de-
tail without degradation. These visual observations are cor-
roborated by the quantitative metrics in Table 2. Although
variants (3) and (4) are visually comparable, (3) achieves
lower errors, validating our design choice.
Ensembling predictions.
We investigate the impact of
ensembling predictions via mean aggregation. The results,
shown in Fig. 6, demonstrate clear benefits but with rapidly
diminishing returns. While a prediction from a single run
achieves a 1.58% improvement on SI log over DA-V2, en-
sembling predictions from 3 runs significantly increases this
to 2.22%. This benefit quickly saturates, with 10 predic-
tions offering only a negligible further gain (2.24%).
6. Conclusion
Re-Depth Anything presents a new method for test-time op-
timization by re-lighting. The key contribution is to use
generative models for scoring the shading-image alignment
instead of requiring a photometric reconstruction. This alle-
viates the need to construct a photorealistic renderer for in-
verse graphics and shows consistent improvements over all
tested datasets and metrics. In future work, we plan to ex-
plore alternative re-synthesis approaches and explore fine-
tuning foundational models at scale on in-the-wild footage.
8


Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised
Re-lighting
Supplementary Material
Input Image
Marigold
DA-V2 (Small)
+
Ours
Frontal
Left Novel Views
Right Novel Views
Test-time
Similar Real
Tiger Image
DA-V2 (Small)
DA-V2 (Giant)
DA-V2 (Giant)
+
Ours
Figure 7. Biased predictions by baselines and our correction via re-lighting. Top: Marigold and the larger DA-V2 variants struggle
with the tiger image in ways similar to the small variant shown in the main paper teaser. Bottom: Our method applies to both the large and
small variants of DA-V2, correcting the overall shape in each case and adding more details when using the large variant.
This supplemental document provides additional quali-
tative and quantitative comparisons, justifies the choice of
DA-V2 as the baseline, and explains the challenge of com-
paring state-of-the-art depth estimation models due to mis-
matching depth representations and evaluation protocols.
7. Additional Comparisons
Fig. 7 exemplifies how different depth estimation methods
all struggle with the tiger image from the main paper. The
reconstructed dog-like shapes indicate a bias in the train-
ing data rather than an issue with the model architecture
or depth representation. Our re-lighting is agnostic to the
training and corrects the bias from a dog-like to a tiger-
like shape, regardless of whether the giant or small variant
of DA-V2 is used. Quantitative improvements over other
methods are listed in Table 3 and discussed further below.
We present additional qualitative results in Figs. 9, 10,
11, and 12. These figures also illustrate several remain-
ing limitations of our method, highlighted by red boxes
and discussed in the main document. Fig. 8 further shows
the per-sample change in SqRel relative to DA-V2, high-
lighting that our method delivers more frequent and larger-
magnitude improvements across all three datasets.
1


CO3D
ETH3D
KITTI
Figure 8. Histograms of per-sample change in SqRel with respect to DA-V2 on samples from CO3D, ETH3D and KITTI. The x-axis shows
‚àÜSqRel = SqRelDA-V2 ‚àíSqRelOurs (for CO3D scaled by 10‚àí5 as indicated on the axis): blue bars on the right correspond to samples where
our method achieves lower SqRel than DA-V2, and orange bars on the left to samples where DA-V2 is better. The dashed vertical line
marks the mean ‚àÜSqRel for each dataset. On CO3D, the distribution is clearly skewed towards positive changes with relatively few and
smaller-magnitude failures. ETH3D and KITTI both exhibit a heavier positive tail. Overall, our method improves SqRel on more samples
than it degrades, and the gains are larger in magnitude than the occasional losses, leading to a positive mean ‚àÜSqRel on all three datasets.
8. Results with the DA-V2 Giant Backbone
We used the ViT-S backbone for the main experiments be-
cause it performs almost as well as the larger ones while
being significantly smaller (in terms of embedding size) and
therefore more efficient to optimize. We also found that the
giant model with the ViT-G encoder suffers from similar bi-
ases, such as the dog-like prediction for the tiger image in
Fig. 7. Therefore, we used the smaller model for develop-
ment and for the major comparisons.
9. Evaluation Protocol Details
Since monocular depth estimation is inherently scale-
ambiguous, various depth representations and correspond-
ing evaluation protocols have been proposed to support
different invariances. Unfortunately, even among relative
depth estimation methods, there are discrepancies in the
evaluation protocols that are used. The classical approach is
to align the relative depth prediction with the ground-truth
depth in a least-squares sense before applying a range of
Table 3. Quantitative comparison with DA-V2 and Marigold. DA-V2 outperforms the other baselines on CO3D, and our method further
improves its performance. We compared different alignment metrics (see Sec. 9) and highlighted the comparable ones in gray and black,
respectively. We advocate using the protocol marked in solid black.
Dataset
Method
Prediction
Higher is better ‚Üë
Lower is better ‚Üì
Rel. Disp.
Rel. Depth
Abs. Depth
Œ¥1
Œ¥2
Œ¥3
AbsRel
RMSE
log10
RMSE log
SI log
SqRel
CO3D
Marigoldls-depth
‚úì
1.0
1.0
1.0
0.00276
0.0685
0.001200
0.00366
0.366
0.000320
Marigoldls-depth-disp
‚úì
1.0
1.0
1.0
0.00275
0.0685
0.001196
0.00366
0.366
0.000320
DepthProls-depth
‚úì
1.0
1.0
1.0
0.00242
0.0625
0.001053
0.00334
0.334
0.000286
DepthProls-depth-disp
‚úì
1.0
1.0
1.0
0.00241
0.0625
0.001046
0.00334
0.334
0.000286
DA-V2ls-disp-depth
‚úì
1.0
1.0
1.0
0.00227
0.0602
0.000985
0.00321
0.321
0.000244
Oursls-disp-depth
‚úì
1.0
1.0
1.0
0.00223
0.0588
0.000968
0.00314
0.314
0.000235
DA-V2ls-disp
‚úì
1.0
1.0
1.0
0.00226
0.0602
0.000981
0.00321
0.321
0.000244
Oursls-disp
‚úì
1.0
1.0
1.0
0.00222
0.0588
0.000964
0.00314
0.314
0.000235
KITTI
Marigoldls-depth
‚úì
0.889
0.978
0.992
0.109
3.86
0.047
0.162
16.1
0.53
Marigoldls-depth-disp
‚úì
0.876
0.953
0.980
0.110
5.63
0.051
0.181
17.9
0.84
DepthProls-depth
‚úì
0.937
0.987
0.995
0.086
2.74
0.037
0.132
13.0
0.30
DepthProls-depth-disp
‚úì
0.896
0.963
0.983
0.096
6.65
0.045
0.186
18.3
3.21
DA-V2ls-disp-depth
‚úì
0.568
0.796
0.902
0.305
7.01
0.118
0.348
33.6
2.49
Oursls-disp-depth
‚úì
0.593
0.818
0.917
0.283
6.71
0.110
0.319
30.7
2.20
DA-V2ls-disp
‚úì
0.818
0.937
0.974
0.323
130
0.062
0.256
25.4
1756
Oursls-disp
‚úì
0.823
0.940
0.976
0.276
105
0.060
0.243
24.1
1335
ETH3D
Marigoldls-depth
‚úì
0.963
0.994
0.998
0.058
0.476
0.0255
0.101
10.0
0.083
Marigoldls-depth-disp
‚úì
0.952
0.988
0.995
0.072
4.666
0.0318
0.178
17.7
30.25
DepthProls-depth
‚úì
0.966
0.993
0.997
0.058
0.498
0.0237
0.077
7.69
0.158
DepthProls-depth-disp
‚úì
0.962
0.990
0.994
0.065
5.489
0.0299
0.206
20.5
19.9
DA-V2ls-disp-depth
‚úì
0.884
0.956
0.978
0.113
0.955
0.0448
0.153
15.1
0.391
Oursls-disp-depth
‚úì
0.898
0.965
0.982
0.104
0.875
0.0413
0.143
14.1
0.347
DA-V2ls-disp
‚úì
0.968
0.991
0.995
0.198
35.69
0.0253
0.0998
9.93
1339
Oursls-disp
‚úì
0.968
0.992
0.996
0.148
23.27
0.0253
0.0941
9.36
850.9
2


metrics, which may include log transformations to reduce
the impact of uncertainties. The benefit of aligning to the
ground truth is that it provides interpretable results in metric
space that are comparable to methods using absolute depth
prediction.
DA-V2 deviated from this path by predicting disparity
instead of relative depth. Hence, they evaluated directly in
disparity space, using the same alignment procedure and
metrics. However, this makes it incomparable to models
that evaluate on depth. An established alternative is to per-
form the alignment in disparity space and then apply the
metrics after converting disparity back to depth. However,
least-squares fitting in disparity handles outliers very differ-
ently than when computed in depth. For instance, in dispar-
ity, far estimates are less pronounced, leading to an unfair
comparison with methods that perform alignment directly
in depth space.
To provide an as-fair-as-possible comparison consistent
with widely used evaluation protocols, we followed the pro-
cedure described in the main document: we first align to
obtain absolute disparity, and then perform a second align-
ment to minimize least-squares errors in the same space
used by methods that output relative depth. We refer to this
as least-squares disparity-and-depth (ls-disp-depth) align-
ment. To shed light on the effect of the different alignment
methods, we compare them in in Tab. 9. The least-squares
alignment performed directly in disparity space followed by
depth conversion (ls-disp), as used in [12], performs better
on CO3D than ls-disp-depth, but worse on the other two
datasets. To further analyze this effect for methods predict-
ing depth, we also mapped depth predictions into disparity
space for a second alignment (ls-depth-disp), and observed
the same trend. This experiment highlights the importance
of the alignment procedure, and we conclude that the fairest
comparison is to perform the alignment in the same (depth)
space, i.e., to use ls-disp-depth for methods operating on
disparity and ls-depth for methods outputting depth directly.
Note that the initial alignment in disparity space (ls-disp-
depth) is inevitable for methods that output disparity, as it is
required to obtain absolute disparity before converting dis-
parity to depth.
10. Baseline Selection
To determine the suitability of DA-V2 as a baseline, we
evaluated several existing methods (including Marigold,
DepthPro, and DA-V2) on the CO3D dataset. We selected
CO3D as our benchmark because its objects exhibit high de-
tail and are relatively close to the camera (e.g., a toy truck
instead of a real truck in KITTI). As a result, the depth esti-
mates are more reliable, which aligns well with our goal of
detail refinement, as motivated in the main document.
On CO3D, DA-V2 consistently outperformed all other
methods. Marigold (diffusion-based foundational model for
Table 4. Ablation of the camera model and b parameter on the
CO3D dataset.
Method
AbsRel ‚Üì
RMSE ‚Üì
log10 ‚Üì
RMSE log ‚Üì
SI log ‚Üì
SqRel ‚Üì
Ours (Korth, b = 0.1)
0.00223
0.0588
0.000968
0.00314
0.314
0.000235
Ours-Perspective-Fixed (Kpersp, b = 0.1)
0.00224
0.0591
0.000973
0.00315
0.315
0.000236
Ours-Perspective-Opt (Kpersp, binit = 0.01)
0.00225
0.0592
0.000976
0.00316
0.316
0.000237
depth estimation) captures coarse geometric structure, its
reconstructed mesh, as shown in Fig. 7, exhibits significant
high-frequency noise and artifacts. DepthPro provides an
estimate of absolute scale, but it is often misled by toy ver-
sions of real objects (e.g., a toy truck), and even after nor-
malizing for scale (and shift), it does not outperform DA-
V2. This justifies our selection of DA-V2 as the baseline.
Notably, Table 3 shows that DA-V2 performs better on
CO3D but slightly worse on the other two datasets. This is
consistent with the evaluations in DepthPro and Marigold,
which report improvements over DA-V2 on this datasets.
These results support the visual observation that DA-V2 ex-
cels in predicting fine details while lacking a bit in capturing
overall scene composition and the relative scale of objects.
Another closely related work is BetterDepth [61], which,
similar to our approach, focuses on refining the output of a
pre-trained depth foundation model. However, their imple-
mentation is not publicly available.
11. Ablation - Perspective Camera
We conduct an ablation study on the camera model (Eq. 2
in the main paper) and the choice of b = ms (Eq. 5 in the
main paper). We compare a scaled orthographic camera,
with its scale factor optimized from an initial value of 7.0,
against a perspective camera, with its focal length jointly
optimized from an initial value of 2.0. As shown in Table 4,
the scaled orthographic model with b = 0.1 yields the low-
est errors. We therefore adopt this configuration for all main
experiments.
12. SfS Implementation Details
We implement a simple shape from shading algorithm sim-
ilar to [11]. We assume a Lambertian surface, with constant
albedo and a single light at infinity such that the light direc-
tion l is unknown yet constant across all pixels. We consider
only brightness variations and therefore convert the input
image to grayscale and set the incoming light intensity to
Lin = max(I), where I is the input image.
Under these assumptions, the image formation model re-
duces to a Lambertian dot product between the surface nor-
mal and the light direction:
  \ha t { \mI
 
}( u,v)  =  \
m
ax \bigl (0,\; \mN (u,v)\cdot \mathbf {l}\bigr ), 
where ÀÜI is the rendered image.
3


Motivated by the shape from intensity gradient tech-
nique [59], we initialize the normals using image gradients,
  \m N (
u
,v) = \bi gl (-\ mI _
u
(u,v),\; -\mI _v(u,v),\; 1\bigr ), 
where Iu and Iv denote the partial derivatives of I with re-
spect to the spatial coordinates u and v. We compute these
derivatives using Scharr filters [38].
To optimize light direction and surface normal, we mini-
mize a combination of a photometric loss and a smoothness
loss,
  \mathca l  {L} = \mathcal {L}_{\text {smooth}} + \lambda \,\mathcal {L}_{\text {photo}}, 
where Œª is a regularization parameter. The photometric loss
minimizes the difference between the input and the rendered
images,
  \mat h c
al 
{
L}_{\te
x
t {p ho t o}} =  \
frac {1}{|\Omega |} \sum _{(u,v)\in \Omega } \bigl (\mI (u,v) - \hat {\mI }(u,v)\bigr )^2, 
where ‚Ñ¶denotes the valid region defined by the object
mask. The smoothness loss penalizes spatial variation in
the normals,
  \math c a
l {
L
}_{\tex
t
 {smoot h}} 
=  \frac { 1}{|
\
O
mega |} \sum _{(u,v)\in \Omega } \left ( \|\nabla \mN _u(u,v)\|_2^{2} + \|\nabla \mN _v(u,v)\|_2^{2} \right ), 
where Nu and Nv denote the first two components of the
normal field.
This simple shading model closely matches the re-
lighting procedure used in our main method, highlighting
the benefit of our shading-based augmentation. Unlike this
full photometric reconstruction, which produces artifacts at
texture boundaries or under complex real-world illumina-
tion, our re-lighting refinement succeeds with a simple and
robust illumination model.
4


Input Image
GT
DA-V2
Ours
DA-V2
Ours
Disparity Maps
Surface Normals
Figure 9. Additional qualitative comparison on the CO3D dataset.
5


Input Image
GT
DA-V2
Ours
DA-V2
Ours
Disparity Maps
Surface Normals
Figure 10. Additional qualitative comparison on the CO3D dataset. Red square highlights occasional oversmoothing, a limitation of
our method.
6


Input Image
GT
DA-V2
Ours
DA-V2
Ours
Disparity Maps
Surface Normals
Figure 11. Additional qualitative comparison on the ETH3D dataset. Red square highlights occasional oversmoothing, a limitation of
our method.
7


Input Image
GT
DA-V2
Ours
DA-V2
Ours
Disparity Maps
Surface Normals
Figure 12. Additional qualitative comparison on the KITTI dataset. Red square highlights hallucination on sky areas, a limitation of
our method.
8


References
[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2020. 2
[2] Ananta R Bhattarai, Xingzhe He, Alla Sheffer, and Helge
Rhodin.
Dreamtexture: Shape from virtual texture with
analysis by augmentation. arXiv preprint arXiv:2503.16412,
2025. 3
[3] James F. Blinn. Models of light reflection for computer syn-
thesized pictures.
In Computer Graphics and Interactive
Techniques, 1977. 2, 3, 4
[4] Aleksei Bochkovskii, Ama√ÉG, l Delaunoy, Hugo Germain,
Marcel Santos, Yichao Zhou, Stephan R Richter, and
Vladlen Koltun. Depth pro: Sharp monocular metric depth in
less than a second. In International Conference on Learning
Representations (ICLR), 2025. 2
[5] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela
Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent
Leroy. Must3r: Multi-view network for stereo 3d reconstruc-
tion. In Computer Vision and Pattern Recognition Confer-
ence (CVPR), 2025. 2
[6] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using
gaussian splatting. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2023. 3
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions (ICLR), 2021. 1
[8] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Neural Information Processing Systems (NeurIPS),
2014. 2
[9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The In-
ternational Journal of Robotics Research, 32:1231 ‚Äì 1237,
2013. 2, 6
[10] Berthold K. P. Horn and Michael J. Brooks.
Shape from
Shading. MIT Press, Cambridge, Massachusetts, 1989. 3
[11] Berthold K. P. Horn and Michael J. Brooks. The variational
approach to shape from shading, page 173‚Äì214. MIT Press,
Cambridge, MA, USA, 1989. 3
[12] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Met-
zger, Rodrigo Caye Daudt, and Konrad Schindler. Repurpos-
ing diffusion-based image generators for monocular depth
estimation.
In Computer Vision and Pattern Recognition
Conference (CVPR), 2024. 2, 5, 3
[13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler,
and George Drettakis.
3d gaussian splatting for real-time
radiance field rendering.
ACM Transactions on Graphics
(TOG), 42:1 ‚Äì 14, 2023. 1, 2
[14] Vincent Leroy, Yohann Cabon, and J√©r√¥me Revaud. Ground-
ing image matching in 3d with mast3r. In European Confer-
ence on Computer Vision (ECCV), 2024. 2
[15] Bo Li, Chunhua Shen, Yuchao Dai, Anton van den Hengel,
and Mingyi He. Depth and surface normal estimation from
monocular images using regression on deep features and hi-
erarchical crfs. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2015. 2
[16] Huan Li, Matteo Poggi, Fabio Tosi, and Stefano Mattoc-
cia.
On-site adaptation for monocular depth estimation
with a static camera. In British Machine Vision Conference
(BMVC), 2023. 2
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.
Hoi.
Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models.
In International Conference on Machine Learning (ICML),
2023. 5
[18] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.
Binsformer: Revisiting adaptive bins for monocular depth
estimation.
IEEE Transactions on Image Processing, 33:
3964‚Äì3976, 2022. 2
[19] Zhi Li, Shaoshuai Shi, Bernt Schiele, and Dengxin Dai.
Test-time domain adaptation for monocular depth estima-
tion. In International Conference on Robotics and Automa-
tion (ICRA), 2023. 2
[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2022. 2, 3
[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In International Confer-
ence on Computer Vision (ICCV), 2023. 3
[22] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan
Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-
Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized
text-to-3d object synthesis. In International Conference on
Computer Vision (ICCV), 2023. 2, 3
[23] R‚Äôemi Marsal, Alexandre Chapoutot, Philippe Xu, and
David Filliat. A simple yet effective test-time adaptation for
zero-shot monocular metric depth estimation. 2024. 2
[24] Luke Melas-Kyriazi, Iro Laina, C. Rupprecht, and Andrea
Vedaldi. Realfusion 360¬∞ reconstruction of any object from a
single image. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 3
[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation of
3d shapes and textures. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2023. 3
[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view
synthesis.
In European Conference on Computer Vision
(ECCV), 2020. 1, 2
[27] Maxime Oquab,
Timoth√©e Darcet,
Th√©o Moutakanni,
Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-
dez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ
Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,
9


Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao
Xu, Herv√© J√©gou, Julien Mairal, Patrick Labatut, Armand
Joulin, and Piotr Bojanowski. Dinov2: Learning robust vi-
sual features without supervision. ArXiv, abs/2304.07193,
2023. 3
[28] Hyoungseob Park, Anjali W. Gupta, and Alex Wong. Test-
time adaptation for depth completion.
In Conference on
Computer Vision and Pattern Recognition (CVPR), 2024. 2
[29] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mat-
tia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool.
Unidepthv2: Universal monocular metric depth estimation
made simpler. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2025. 2
[30] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim
Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach.
Sdxl: Improving latent diffusion models for high-resolution
image synthesis. In International Conference on Learning
Representations (ICLR), 2023. 2
[31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In In-
ternational Conference on Learning Representations (ICLR),
2023. 2, 3
[32] Ren√© Ranftl,
Katrin Lasinger,
David Hafner,
Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. Transactions on Pattern Analysis and Machine In-
telligence, 44:1623‚Äì1637, 2019. 1, 2
[33] Ren√© Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In International Con-
ference on Computer Vision (ICCV), 2021. 1, 2, 3
[34] Alex Rasla and Michael Beyeler. The relative importance
of depth cues and semantic edges for indoor mobility us-
ing simulated prosthetic vision in immersive virtual reality.
ACM Symposium on Virtual Reality Software and Technol-
ogy, 2022. 1
[35] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
real-life 3d category reconstruction. In International Con-
ference on Computer Vision (ICCV), 2021. 6
[36] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick
Esser, and Bj√∂rn Ommer. High-resolution image synthesis
with latent diffusion models. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2021. 2, 5
[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mah-
davi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho,
David J. Fleet, and Mohammad Norouzi. Photorealistic text-
to-image diffusion models with deep language understand-
ing. In Neural Information Processing Systems (NeurIPS),
2022. 2
[38] Hanno Scharr. Optimal operators in digital image process-
ing. PhD thesis, University of Heidelberg, Germany, 2000.
4
[39] Thomas Schops, Torsten Sattler, and Marc Pollefeys. Bad
slam: Bundle adjusted direct rgb-d slam. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2019. 6
[40] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu,
and Zhengguo Li.
Nddepth:
Normal-distance assisted
monocular depth estimation. In International Conference on
Computer Vision (ICCV), 2023. 2
[41] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus.
Indoor segmentation and support inference from
rgbd images. In European Conference on Computer Vision
(ECCV), 2012. 2
[42] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. Dreamgaussian: Generative gaussian splatting for ef-
ficient 3d content creation. In International Conference on
Learning Representations (ICLR), 2023. 3
[43] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,
and Gregory Shakhnarovich. Score jacobian chaining: Lift-
ing pretrained 2d diffusion models for 3d generation.
In
Conference on Computer Vision and Pattern Recognition
(CVPR), 2022. 2, 3
[44] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang,
Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking
accurate monocular geometry estimation for open-domain
images with optimal training supervision. In Computer Vi-
sion and Pattern Recognition Conference (CVPR), 2025. 2
[45] Shuzhe Wang,
Vincent Leroy,
Yohann Cabon,
Boris
Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi-
sion made easy. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2024. 2
[46] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-
ran, Mark E. Campbell, and Kilian Q. Weinberger. Pseudo-
lidar from visual depth estimation: Bridging the gap in 3d
object detection for autonomous driving. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 1
[47] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distilla-
tion. In Neural Information Processing Systems (NeurIPS),
2023. 2, 3
[48] Ryan White and David A Forsyth. Combining cues: Shape
from shading and texture. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2006. 3
[49] Andrew P. Witkin. Recovering surface shape and orientation
from texture. Artificial Intelligence, 17(1-3):17‚Äì45, 1981. 3
[50] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman,
and Vivienne Sze. Fastdepth: Fast monocular depth estima-
tion on embedded systems. In International Conference on
Robotics and Automation (ICRA), 2019. 1
[51] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe L.
Lin, and ZHIGUO CAO. Structure-guided ranking loss for
single image depth prediction. In Computer Vision and Pat-
tern Recognition (CVPR), 2020. 2
[52] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi
Feng, and Hengshuang Zhao. Depth anything: Unleashing
the power of large-scale unlabeled data. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2024. 2
[53] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xi-
aogang Xu, Jiashi Feng, and Hengshuang Zhao.
Depth
anything v2.
In Neural Information Processing Systems
(NeurIPS), 2024. 1, 2, 3, 6
10


[54] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren.
Gedepth:
Ground embedding for monocular depth esti-
mation.
In International Conference on Computer Vision
(ICCV), 2023. 2
[55] Chongjie Ye, Yinyu Nie, Jiahao Chang, Yuantao Chen, Yi-
hao Zhi, and Xiaoguang Han.
Gaustudio:
A modular
framework for 3d gaussian splatting and beyond.
ArXiv,
abs/2403.19632, 2024. 1
[56] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-
forcing geometric constraints of virtual normal for depth pre-
diction.
In International Conference on Computer Vision
(ICCV), 2019. 2
[57] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,
Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen.
Met-
ric3d: Towards zero-shot metric 3d prediction from a sin-
gle image. In International Conference on Computer Vision
(ICCV), 2023. 2
[58] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. Neural window fully-connected crfs for monocular
depth estimation. In Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3906‚Äì3915, 2022. 2
[59] Ruo Zhang and Mubarak Shah. Shape from intensity gradi-
ent. IEEE Transactions on Systems, Man, and Cybernetics -
Part A: Systems and Humans, 29(3):318‚Äì325, 1999. 4
[60] Ruo Zhang, Ping-Sing Tsai, James Edwin Cryer, and
Mubarak Shah. Shape-from-shading: a survey. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 21(8):
690‚Äì706, 1999. 3
[61] Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando
Metzger, Anton Obukhov, Markus Gross, Konrad Schindler,
and Christopher Schroers. Betterdepth: Plug-and-play dif-
fusion refiner for zero-shot monocular depth estimation. In
Neural Information Processing Systems (NeurIPS), 2024. 2,
3
[62] Yizhou Zhao, Hengwei Bian, Kaihua Chen, Pengliang Ji,
Liao Qu, Shao yu Lin, Weichen Yu, Haoran Li, Hao Chen,
Jun Shen, Bhiksha Raj, and Min Xu.
Metric from hu-
man: Zero-shot monocular metric depth estimation via test-
time adaptation. In Neural Information Processing Systems
(NeurIPS), 2024. 2
[63] Junzhe Zhu, Peiye Zhuang, and Oluwasanmi Koyejo. Hifa:
High-fidelity text-to-3d generation with advanced diffusion
guidance. In International Conference on Learning Repre-
sentations (ICLR), 2023. 2, 3
11
