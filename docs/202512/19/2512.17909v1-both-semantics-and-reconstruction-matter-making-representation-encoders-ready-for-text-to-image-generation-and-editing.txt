Both Semantics and Reconstruction Matter:
Making Representation Encoders Ready for
Text-to-Image Generation and Editing
Shilong Zhang1 He Zhang2 Zhifei Zhang2 Chongjian Ge2 Shuchen Xue3
Shaoteng Liu2 Mengwei Ren2 Soo Ye Kim2 Yuqian Zhou2 Qing Liu2 Daniil Pakhomov2
Kai Zhang2 Zhe Lin2 Ping Luo1
1The University of Hong Kong
2Adobe Research
3University of Chinese Academy of Sciences
Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent
spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding,
a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents.
However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature
space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate
object structures; and (2) the encoder‚Äôs inherently weak pixel-level reconstruction hinders the generator from
learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt
understanding-oriented encoder features for generative tasks. We introduce a semantic‚Äìpixel reconstruction
objective to regularize the latent space, enabling the compression of both semantic information and fine-grained
details into a highly compact representation (96 channels with 16 √ó 16 spatial downsampling). This design
ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction,
while remaining compact enough for accurate generation. Leveraging this representation, we design a unified
Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate
that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains
in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust
generative components.
Date: December 20th, 2025
Project Page: https://jshilong.github.io/PS-VAE-PAGE/
1
Introduction
Representation encoders trained via self-supervision (Caron et al., 2021; Oquab et al., 2023; Sim√©oni et al., 2025;
He et al., 2019, 2022) or contrastive learning (Radford et al., 2021; Tschannen et al., 2025; Bolya et al., 2025) have
established themselves as the cornerstone of visual understanding. They produce highly discriminative, semantic-rich
features that generalize exceptionally well, enabling efficient adaptation to downstream tasks with limited data (Liu et al.,
2023, 2024a). From dense prediction tasks to complex reasoning in Large Vision Language Models, these encoders have
become the universal bedrock of visual analysis. Yet, despite this pervasive dominance, these powerful representations
have yet to conquer the generative domain.
Instead, state-of-the-art generative systems predominantly rely on Variational Autoencoder (VAE) (Kingma and Welling,
2013), which operates on low-level, compact latents trained with a pixel reconstruction objective. These VAE latents
lack the high-level semantic structure of representation encoders, forcing diffusion models to learn visual concepts
from scratch and necessitating massive computational resources (Esser et al., 2024). To bridge this divide and achieve
the long-sought goal of unifying perception and generation, a natural question arises: can we migrate the generative
modeling space from VAE latents to the representation-encoder space, enabling diffusion models to directly benefit from
the representation encoder‚Äôs discriminative and semantically structured features?
Recent work, RAE (Zheng et al., 2025), offers a pioneering answer to this question. By redesigning the DiT architecture
to handle high-dimensional features, it successfully enables generation within the representation space, achieving
impressive results on the class-conditional ImageNet benchmark (Russakovsky et al., 2015). However, the efficacy of
1
arXiv:2512.17909v1  [cs.CV]  19 Dec 2025


this paradigm does not easily translate to open-world applications. When extended to practical text-to-image synthesis
and complex instruction-based editing tasks, RAE exhibits significant performance limitations compared to mature
VAE-based baselines, as highlighted in Figure 1.
40k
60k
80k
100k
120k
140k
160k
180k
200k
Training Iterations (k)
55
60
65
70
75
80
GenEval Score (%)
PS-VAE: 76.56
VAE: 75.75
S-VAE: 75.00
RAE: 71.30
VAE
RAE
S-VAE
PS-VAE
rFID
PSNR
LPIPS
SSIM
VAE
0.534
26.18
0.135
0.715
RAE
0.619
19.20
0.254
0.436
S-VAE
1.407
17.78
0.296
0.390
PS-VAE
0.203
28.79
0.085
0.817
Reconstruction Metrics
Figure 1
Reconstruction and generation performance
across different generation spaces. Compared to vanilla VAE,
RAE improves generation coverage speed but quickly saturates due
to its unconstrained semantic space and weak reconstruction. To
address this, we project RAE features into a compact 96-channel
latent space with a semantic reconstruction objective, forming
S-VAE, which mitigates off-manifold issues and improves genera-
tion performance. Finally, PS-VAE further augments the semantic
latent space with pixel-level reconstruction, enriching structural
and texture details and achieving superior performance in both
reconstruction and generation.
To uncover the root causes of this degradation, we
analyze the behavior of the representation space both
experimentally and theoretically in Section 3.2, identi-
fying two key issues: insufficient compact regulariza-
tion of representation features, leading to off-manifold
latent generation, together with weak pixel-level recon-
struction, which prevents the generator from learning
accurate geometry and texture.
The first issue is that generation over representation
features is performed in an unconstrained space without
compact regularization, leading to a mismatch between
the high dimensionality of representation features and
their much lower intrinsic information content. Train-
ing on such a redundant high-dimensional space makes
the diffusion model prone to producing off-manifold
latents1, ultimately leading to inaccurate and structurally
distorted objects. We verify this phenomenon by vi-
sualizing off-manifold outliers through a toy fitting
experiment and a theoretical analysis (Section 3.2 and
Figure 3). This observation motivates us to regularize
the generative space: we propose S-VAE, which maps
the frozen representation features into a compact, KL-
regularized latent space (Rombach et al., 2022) via a
semantic autoencoder. This constraint effectively elim-
inates off-manifold outliers, ensuring that generated latents remain within the valid decoding manifold and thereby
improving generation performance (as shown by the purple line in Figure 1).
The second issue arises from the training objective of representation encoders, which focuses on producing sufficiently
discriminative features for understanding rather than preserving detailed structure and fine-grained visual information
required for generation. Consequently, even within the regularized S-VAE space, the model struggles to synthesize
realistic fine-grained textures and precise small-scale structures. To address this, we unfreeze the encoder and jointly
optimize it with a pixel-level reconstruction loss on the input image and a semantic reconstruction loss defined on the
outputs of the original frozen pretrained encoder. This encourages the encoder to maintain fine-grained details during
the computation of strong semantic representations, yielding our final Pixel‚ÄìSemantic VAE (PS-VAE Figure 1).
Specifically, we instantiate PS-VAE with a 96-channel latent design based on DINOv2 (Oquab et al., 2023). Compared
to vanilla VAEs such as MAR-VAE (Li et al., 2024), this architecture achieves state-of-the-art reconstruction quality,
significantly improving rFID (0.534 ‚Üí0.203), PSNR (26.18 ‚Üí28.79), and SSIM (0.715 ‚Üí0.817). It also outperforms
MAR-VAE in text-to-image generation, exhibiting faster convergence and superior final performance (GenEval (Ghosh
et al., 2023): 75.8 ‚Üí76.6; DPG-Bench (Hu et al., 2024): 83.2 ‚Üí83.6). Most notably, on the challenging instruction-
based image editing task‚Äîrequiring both accurate image understanding and faithful instruction execution‚ÄîPS-VAE
delivers a substantial improvement, boosting the editing reward from 0.06 to 0.22. We also validate our method on
SigLIP2 (Tschannen et al., 2025), which is used in Bagel (Deng et al., 2025), observing consistent generation behavior.
Importantly, the fine-tuned encoder retains strong understanding ability without any LLM fine-tuning, highlighting the
potential of our approach for unifying the encoder for both understanding and generation.
In summary, our contributions are:
‚Ä¢ Conceptual insight. We are the first to show that standard representation encoders are not directly suitable for
text-to-image generation or instruction-based editing. Through comprehensive analysis, we identify two fundamental
issues: off-manifold generation arising from unconstrained feature spaces and poor pixel reconstruction fidelity
1We define ‚Äúoff-manifold‚Äù latents as features falling into undefined/OOD regions where image decoding becomes unreliable.
2


inherent to discriminative pre-training.
‚Ä¢ Methodological design. We propose a principled approach that transforms the original unconstrained feature space
into a compact generative latent space using a semantic autoencoder, and further enriches fine-grained structural and
textural details through a pixel-reconstruction objective. The resulting 96-channel latent space, built on DINOv2-B,
achieves state-of-the-art performance in both reconstruction and generation, and generalizes effectively to SigLIP2,
supporting its potential as a unified encoder for understanding and generation.
‚Ä¢ Unified evaluation. We establish a standardized evaluation pipeline for fair and controlled comparison across
different generation space designs on text-to-image and image-editing tasks, and empirically demonstrate the superior
performance of PS-VAE under this framework.
2
Related work
Representation Encoders for Visual Understanding Representation learning is foundational to modern visual
understanding. By mapping raw image data into a discriminative feature space, pre-trained encoders (such as DINO (Caron
et al., 2021; Oquab et al., 2023; Sim√©oni et al., 2025), SigLIP (Zhai et al., 2023; Tschannen et al., 2025), and Perception
Encoder (Bolya et al., 2025)) enable a wide array of downstream tasks, including classification, object detection,
segmentation, and Vision-Language multi-modality modeling (Liu et al., 2023; Tong et al., 2024). These powerful
encoders are typically obtained through two major paradigms: self-supervised learning (Chen et al., 2020; He et al.,
2019; Grill et al., 2020; Caron et al., 2021; Oquab et al., 2023; Sim√©oni et al., 2025) and image‚Äìtext contrastive
learning (Radford et al., 2021; Zhai et al., 2023; Bolya et al., 2025). Critically, since these representations are optimized
mainly for discrimination, they effectively act as lossy compressors, discarding high-frequency pixel details that are
non-essential for semantic understanding but vital for accurate synthesis. Furthermore, the resulting feature space is
typically high-dimensional and unconstrained (non-regularized). This lack of generative regularization makes them
susceptible to the off-manifold generation issue when used directly as a diffusion target (as shown by our analysis
in Section 1), which prevents their straightforward adaptation to generative modeling tasks.
VAEs for Visual Generation Variational Autoencoders (VAEs) (Kingma and Welling, 2013) are fundamental
components of Latent Diffusion Models (LDMs) (Rombach et al., 2022), primarily serving to reduce the computational
cost of high-resolution generation. However, VAEs are trained mainly on a pixel-reconstruction objective, which often
yields a latent space focused predominantly on low-level structural details rather than high-level semantic concepts. This
forces the subsequent diffusion model to learn complex visual concepts from scratch, necessitating massive computational
resources. While early LDM work evaluated VAEs almost exclusively via reconstruction fidelity (Rombach et al., 2022),
more recent studies have recognized that the topological properties of the latent space are crucial for robust generation.
This realization has prompted efforts to introduce explicit regularization (Kouzelis et al., 2025; Skorokhodov et al., 2025;
Yao et al., 2025), often via KL-divergence constraints, to encourage better latent utilization and stability. Our work
extends this idea by explicitly designing a compact, KL-regularized latent space that is directly informed by high-level
semantics, thus combining the generative stability of VAEs with the discriminative power of foundation models.
Unifying Feature Spaces for Generation and Understanding Recent efforts to bridge the gap between discriminative
and generative feature spaces generally follow two distinct strategies. The first focuses on aligning standard VAE
latents with representation encoders through representation-alignment objectives, treating the encoder as a soft semantic
constraint (e.g., (Yao et al., 2025; Xu et al., 2025)). The second strategy, which is more closely related to this work,
aims to construct a generative feature space directly from representation encoders (Chen et al., 2025; Lu et al., 2025;
Yue et al., 2025; Shi et al., 2025; Zheng et al., 2025).
(Chen et al., 2025; Lu et al., 2025; Yue et al., 2025) do
not explicitly require the latent space to fully preserve the original semantic structure of the representations. More
closely related to our study, SVG (Shi et al., 2025) and RAE (Zheng et al., 2025) propose diffusing directly over raw,
unconstrained, high-dimensional semantic features. While this approach captures powerful semantics, we identify two
critical failures preventing its widespread application: insufficient compact regularization of representation features,
leading to off-manifold latent generation, together with weak pixel-level reconstruction, which prevents the generator
from learning accurate geometry and texture. Thus, we propose PS-VAE, which introduces a principled intermediate
step: creating a compact, KL-regularized semantic latent space and then applying a joint pixel-reconstruction objective
to enrich it with high-fidelity details. This design achieves superior performance in tasks requiring both semantic
understanding and precise structural control, e.g. instruction-based image editing. Related work in the autoregressive
paradigm (Ma et al., 2025; Song et al., 2025; Lin et al., 2025; Han et al., 2025) is discussed in the Supplementary
Material, as it follows a fundamentally different modeling approach.
3


A traditional Venetian mask with intricate designs 
and feather embellishments is ‚Ä¶
Two square-shaped pink erasers rest on ‚Ä¶
RAE
VAE
RAE
VAE
RAE
VAE
(c) Text to Image Comparison
(a) Reconstruction Comparison
RAE
VAE
Remove the elderly man wearing glasses
RAE
VAE
(b) Editing Comparison
RAE
VAE
Two snowboards are positioned side by side on a ‚Ä¶
Figure 2 Visualization comparison between RAE and VAE. (a) RAE shows a noticeable gap in reconstruction performance
compared to VAE. Benefiting from its rich semantic representation, RAE demonstrates stronger prompt-following ability in image
editing tasks that require understanding the input image (b). However, its poor reconstruction quality limits practical usability, as
it fails to preserve fine-grained and consistent details from the input image. Counterintuitively, in text-to-image generation, RAE
exhibits severe structural and texture artifacts and substantially lags behind VAE (c), with a performance gap far larger than that
observed in reconstruction.
3
Method
3.1
Overview
In this section, we begin by providing a detailed analysis of the reconstruction and generation behavior of RAE (Zheng
et al., 2025). We theoretically and experimentally validate that its extremely high-dimensional feature space makes the
diffusion model prone to generating off-manifold latent features that lie outside the training support of the pixel decoder.
Furthermore, RAE‚Äôs inherently poor reconstruction quality hinders the generative model from learning accurate object
structures and fine-grained textures, making it unsuitable for high-fidelity tasks such as instruction-based editing. To
address these fundamental challenges, we subsequently introduce our step-wise strategy for preparing the representation
encoder for generation by mapping both pixels and representations into a unified, compact latent space. Finally, we
present a Deep-Fusion architecture for text-to-image generation and image editing, establishing a fair benchmarking
framework for different generative latent spaces. Unless otherwise specified, we follow the settings of RAE (Zheng et al.,
2025) and use a DINOv2-B (Oquab et al., 2023) encoder for feature extraction.
3.2
Analysis of RAE
Comparison: RAE vs. VAE To analyze the reconstruction and generation behavior of RAE (Zheng et al., 2025), we
conduct a series of visualization and benchmarking experiments, comparing it with a vanilla VAE (Li et al., 2024) using
the same 16 √ó 16 spatial compression. We first compare the reconstruction performance. As shown in Figure 2(a), RAE
exhibits a notable shortfall in reconstruction quality compared to VAE, often introducing artifacts in regions such as
faces and text. This aligns with the quantitative findings in Table 4: although RAE achieves a comparable rFID, its
SSIM and PSNR are significantly lower. This behavior is expected, as the DINOv2 encoder used in RAE is trained with
a purely discriminative objective and does not explicitly optimize for reconstruction. To further validate their impact on
generation, we conduct both text-to-image generation and image editing tasks (All equipped with a wide DDT head as in
RAE (Wang et al., 2025; Zheng et al., 2025)). Notably, in text-to-image generation, despite faster coverage (as shown
in Figure 1) enabled by its strong semantic feature space, RAE still suffers from severe structural and texture artifacts
(see Figure 2(c)) and substantially underperforms VAE, resulting in a much poorer performance on benchmarks such as
GenEval (Ghosh et al., 2023). For image editing tasks, RAE demonstrates a superior capacity for prompt-following in
4


Dim=2
(b) Top 5% (Farthest) Nearest-Neighbor Distance
(a) Off-Manifold Amplification: 
2D Intrinsic Manifold vs. 8D Ambient Space 
Training Step
Dim=8
Figure 3 Off-manifold behavior varies significantly with feature dimensionality. We construct a 2D ‚ÄòPS‚Äô-shaped distribution
and embed it into an 8D ambient space, yielding two learning settings with intrinsic dimension 2 and ambient dimension 8. (a) The
8D setting produces substantially more off-manifold samples than the intrinsic 2D space. (b) We measure the mean nearest-neighbor
distance of the top 5% tail samples and observe that samples generated in 8D deviate much farther from the data manifold, indicating
stronger off-manifold drift.
edits that necessitate semantic comprehension of the input image (see Figure 2(b)), but its poor reconstruction quality
limits detail preservation.
In conclusion, we empirically observed that RAE exhibits inferior reconstruction quality compared to VAE, limiting its
ability to preserve fine-grained details in both image generation and editing. Conversely, its semantically rich latent space
facilitates faster coverage in text-to-image generation and superior prompt-following in editing tasks. While these results
largely align with expectations, we noted one counterintuitive finding: objects generated based on RAE space suffer
from severe artifacts compared to VAE, the severity of which far exceeds what would be predicted by the reconstruction
gap alone.
Analysis of Off-Manifold Behavior in RAE The structural and texture artifacts in RAE far exceed typical reconstruction
errors, suggesting a fundamental problem in the generation process. We hypothesize that these artifacts arise because the
diffusion model, trained on the high-dimensional RAE feature space, generates off-manifold samples. These samples
reside outside the training distribution of the pixel decoder, leading to the sub-optimal decoded results. We trace the root
cause to the discrepancy between the high dimensionality of DINOv2 features and their lower intrinsic information
content.
To rigorously analyze the difficulty of learning a low-dimensional intrinsic manifold embedded in a high-dimensional
space, we model the generative dynamics of an h-dimensional feature space containing an l-dimensional manifold (h > l).
Let z ‚ààRl denote the latent data and x = Qz ‚ààRh denote the observed data, where Q ‚ààRh√ól is a column-orthonormal
mapping (Q‚ä§Q = Il). The forward diffusion processes for x and z are coupled: xt = (1 ‚àít)x0 + tœµh implies that the
projected variable zt = Q‚ä§xt follows zt = (1 ‚àít)z0 + tœµl, where œµl = Q‚ä§œµh. Beyond the coupling of the forward
processes, the optimal denoising objectives are strictly related. We denote the optimal velocity estimators for the intrinsic
and embedded processes as vz,Œ∏ and vx,Œ∏, respectively. These are defined as the expected velocity targets given the noisy
states:
vz,Œ∏(zt) = E[œµl ‚àíz0|zt],
vx,Œ∏(xt) = E[œµh ‚àíx0|xt]
By projecting the signal in the embedded high dimensions onto the data manifold and its orthogonal complement, we can
express the high-dimensional estimator vx,Œ∏ purely in terms of the low-dimensional estimator vz,Œ∏ plus a residual term:
vx,Œ∏(xt) = Qvz,Œ∏(Q‚ä§xt) + 1
t (I ‚àíQQ‚ä§)xt
(1)
Equation 1 reveals a fundamental disparity in learning difficulty. The first term represents the generative flow along the
intrinsic manifold. While this component is intrinsically low-dimensional, the model operating in the ambient space must
implicitly learn the projection (Q‚ä§) and embedding (Q) operations to resolve it. This imposes a significant burden of
manifold discovery‚Äîidentifying the sparse data subspace within the vast high-dimensional ambient space‚Äîa challenge
that is entirely bypassed when diffusing directly in the intrinsic latent coordinates.
The second term consists purely of Gaussian noise in the orthogonal subspace, which forces the network to learn an
identity-like mapping that conveys no semantic information, resulting in inefficient use of model capacity. This also
5


helps explain the behavior observed in RAE (Zheng et al., 2025): when the model dimensionality is smaller than the
input dimensionality, the network struggles to fit even a single example. High-dimensional Gaussian noise is full-rank
and cannot be compressed without loss, which forces the model to allocate sufficient capacity to transmit noise. As a
result, an intrinsic information bottleneck emerges when the model width is smaller than the ambient feature dimension.
This also explains why the wide DDT-Head design (Wang et al., 2025; Zheng et al., 2025), which incorporates a long
skip connection from the input x, substantially improves the performance of RAE.
To validate this theoretical analysis, we investigate diffusion training in a high-dimensional feature space (h = 8) that
implicitly contains a lower-dimensional intrinsic manifold (l = 2). We construct a ground-truth 2D ‚ÄúPS‚Äù-shaped
distribution z and embed it into R8 via a linear isometric mapping x = Qz, where Q ‚ààR8√ó2 has orthonormal columns.
In this setup, Q acts as the linear decoder defining the manifold. We then train identical 256-channel MLP-based diffusion
models separately on the intrinsic 2D data and the embedded 8D data. For evaluation, we project the generated 8D
samples back to the 2D plane using the linear encoder Q‚ä§(noting that Q‚ä§Q = I2, which perfectly recovers on-manifold
data). As shown in Figure 3, learning in the 8D ambient space results in slower convergence and a degradation in sample
quality. Nearest-neighbor distance evaluation against the ground truth reveals that the top 5% tail samples from the 8D
model deviate significantly from the true manifold. Despite sharing the same intrinsic geometry, the unconstrained
high-dimensional representation amplifies off-manifold behavior. This confirms that discovering and training on the
intrinsic low-dimensional isomorphic distribution is essential for stabilizing diffusion training and eliminating generation
artifacts.
3.3
Make Representation Encoders Ready
Building on our analysis of RAE (Zheng et al., 2025), we first address the off-manifold problem, identified as the primary
limitation. Subsequently, we enhance reconstruction fidelity to improve the text-to-image performance and enable
detail-sensitive applications such as image editing.
The overall training framework of PS-VAE is illustrated in Figure 5. Given an input image Iinput ‚ààRH√óW √ó3, we first
extract a semantic feature map f ‚Ä≤
h ‚ààR ÀÜ
H√ó ÀÜ
W √ódh using a pretrained Representation Encoder, e.g. DINOv2-B (Oquab
et al., 2023). As discussed in Section 3.2, f ‚Ä≤
h is a high-dimensional, unconstrained representation, where its dimension
dh = 768.
RAE
S-VAE
PS-VAE
Figure 4 Visual comparison of generated exam-
ples across progressively improved latent spaces
(RAE ‚ÜíS-VAE ‚ÜíPS-VAE). Artifacts are grad-
ually reduced, with step-by-step improvements in
texture and structure.
To address the off-manifold problem, we introduce a semantic
VAE (S-VAE) that maps the high-dimensional unconstrained fea-
ture space f ‚Ä≤
h to a compact latent space fl ‚ààR ÀÜ
H√ó ÀÜ
W √ódl via an
encoder Es. Here, dl = 96 (dl ‚â™dh). Then, another semantic
decoder Ds is adopted to reconstruct the latent back to the original
feature f ‚Ä≤‚Ä≤
h. Both the semantic encoder Es and decoder Ds are
optimized with a semantic reconstruction loss Ls, which combines
an ‚Ñì2 loss and a cosine similarity loss on features, while the latent
is further regularized by a Kullback‚ÄìLeibler divergence loss LKL
following (Rombach et al., 2022). The encoder and decoder share a
symmetric design with three Transformer blocks inherited from the
representation encoder and an MLP projection layer for dimension-
ality adjustment. For the evaluation of S-VAE, we additionally train
a pixel decoder that reconstructs the output image Ioutput from the
detached semantic latent fl.detach() via the pixel reconstruction
loss LP (The pixel reconstruction loss LP follows (Rombach et al.,
2022).). A diffusion model is further trained on this semantic VAE
(S-VAE) latent space. As shown in Figure 4 and Table 4, both visual
quality and quantitative results are substantially improved, despite
a slight performance drop in reconstruction fidelity. This result
confirms that the primary limitation lies in the off-manifold issue
rather than reconstruction quality.
To enhance image reconstruction without compromising the semantic structure of the latent space, we unfreeze the
representation encoder during pixel decoder training. By removing the detach operation in fl, we enable gradients to
propagate from the pixel decoder back to the encoder. To preserve the pretrained semantic representations during this
6


Figure 5 Compact latent space construction for preserving semantic structure and fine-grained details We first
regularize the unconstrained representation-encoder feature space by freezing the encoder and training a semantic VAE using only the
Ls and Lkl; during this stage, the pixel decoder is trained on the detached semantic latent with pixel reconstruction loss LP. After
semantic reconstruction converges, we unfreeze all components and allow the pixel decoder to backpropagate the gradient into the
encoder, ensuring that the representation encoder captures fine-grained details of the input image.
optimization, we enforce a semantic reconstruction loss on f ‚Ä≤
h and f ‚Ä≤‚Ä≤
h relative to the original encoder, while retaining the
KL loss and pixel-reconstruction loss. After this training stage, we obtain our Pixel-Semantic VAE (PS-VAE).
As demonstrated in Figure 4 and Table 4, this strategy significantly improves the reconstruction quality of the
representation encoder while preserving its semantic structure. This enables the generation model to learn fine-grained
geometry and texture, while the well-preserved semantics ensure fast coverage of text-to-image pretraining and strong
instruction-following ability for image editing(as shown in Figure 7). As a result, both visual quality and quantitative
performance are consistently enhanced for text-to-image generation and editing.
3.4
Generation Architecture
(a) LlamaFusion
(c) Transfusion
(b) Bagal
Text Block
Image Block
FFN
‚ùÑ
QKV
‚ùÑ
FFN
üî•
QKV
üî•
Self-Attention
FFN
üî•
QKV
üî•
FFN
üî•
QKV
üî•
Self-Attention
Text Block
Image Block
FFN
üî•
QKV
üî•
Self-Attention
Text-Image Shared Block
Figure 6 Block comparison of three deep-fusion architectures.
Unified models for generation and understanding are
being actively explored. Owing to its strong semantic
representation and high-fidelity reconstruction, PS-VAE
has strong potential to serve as a unified encoder in such
frameworks. For these reasons, we adopt a deep-fusion
architecture as our generation paradigm. To investigate
which deep-fusion architecture yields superior perfor-
mance, we first conduct a preliminary ablation study on
three popular deep-fusion designs for generation, as illustrated in Figure 6. (a) LlamaFusion (Shi et al., 2024), which
freezes all language blocks and adds parallel image blocks with identical architecture; (b) Bagel-style models (Deng
et al., 2025), which unfreeze both text and image branches to improve multimodal alignment; and (c) Transfusion (Zhou
et al., 2025), which processes image and text tokens jointly using fully shared transformer blocks.
Table 1 GenEval scores of Deep-Fusion architectures.
Model
Params (M)
GenEval ‚Üë
LlamaFusion
857
0.524
Bagel
857
0.763
Transfusion
500
0.752
Transfusion + Wide DDT Head
653
0.762
We evaluate the three deep-fusion architectures using
VAVAE (Li et al., 2024) (32-channel latent, stride-16,
patch size 1). All fusion blocks are initialized from
Qwen2.5-0.5B (Bai et al., 2023). We apply 2D posi-
tional encoding to the VAE features and inject timestep
embeddings into their initial hidden states before feeding
them into the LLM backbone, following Bagel (Deng
et al., 2025). For text-to-image, we concatenate text
embeddings with the noisy image latent. Text tokens use a causal mask, while noisy image latent uses full attention mask.
For image editing, we concatenate the clean latents of input images, the instruction text embeddings, and the noisy latents.
We apply a full attention mask to the clean and noisy latents, while employing a causal mask for the instruction texts.
7


Results in Table. 1 indicate that LlamaFusion exhibits a clear bottleneck, likely due to its frozen language branch being
unable to adapt to text-to-image generation. Compared to the Transfusion-style block design, the Bagel-style design
improves performance by 1.1 but increases parameters by 71%. Since we only evaluate text-to-image performance across
different feature spaces and do not consider preserving language modeling capability, we adopt the TransFusion-style
block as our core fusion architecture for better parameter efficiency.
With the fusion block fixed, we incorporate the wide DDT head (Wang et al., 2025) from RAE (Zheng et al., 2025), which
enhances generation quality in high-channel feature spaces(as we analyzed in Section 3.2). We validate its effectiveness
through consistent gains across multiple VAEs. As shown in Table 1, the head improves VAVAE (32-channel, stride-16,
patch size 1) from 75.2 to 76.2. We observe similar improvements for Flux-VAE (Labs, 2024) (16-channel, stride-8,
patch size 2), which increases from 63.7 to 68.04, and MAR-VAE (16-channel, stride-16, patch size 1), which rises from
72.6 to 75.75. Given these consistent results, we adopt the wide DDT head as a standard component.
4
Experiments
In this subsection, we outline our training and inference pipelines, followed by the evaluation protocols for reconstruction,
text-to-image generation, and instruction-based image editing. We then present performance results across varying
feature spaces to demonstrate the effectiveness of PS-VAE. Subsequently, we analyze the scaling behavior of our 96- and
32-channel variants, showing that larger generation models effectively leverage the rich semantic and pixel-level details
preserved in high-channel latent spaces. Finally, we extend our framework by replacing the DINOv2 (Oquab et al., 2023)
encoder with SigLIP2 (Tschannen et al., 2025), highlighting the latter‚Äôs potential as a unified encoder for both visual
understanding and generative modeling.
4.1
Training and Evaluation Details
Reconstruction. To ensure a fair comparison with prior work, we train our reconstruction models exclusively on
ImageNet-1K (Russakovsky et al., 2015), though we note that future work could benefit from larger, more diverse
datasets. Input images are resized and center-cropped to 224 √ó 224. Using a patch size of 14 results in a sequence
length of 16 √ó 16, making the computation‚Äîin terms of both FLOPs and runtime‚Äîsignificantly more efficient than the
VAE-style encoders used in Latent Diffusion Models (LDM) (Rombach et al., 2022). Our pixel decoder adopts the LDM
architecture (Rombach et al., 2022) and reconstructs images at a resolution of 256 √ó 256. We evaluate performance
using rFID, SSIM, PSNR, and LPIPS on the ImageNet-1K validation set. Models are trained with a batch size of 96 and
a learning rate of 10‚àí4. We employ a two-stage training strategy: first, we freeze the foundation model and train only the
semantic encoder and decoder, with the pixel decoder trained on detached semantic latents to prevent interference with
semantic compression. In the second stage, we unfreeze all components, allowing gradients from the pixel decoder to
backpropagate to both the foundation model and the semantic encoder. The loss weights for Ls and Lp are set to 1 and
0.1, respectively. Further details are provided in the Supplementary Material.
Text-to-Image Generation. We utilize CC12M-LLaVA-NeXT (Changpinyo et al., 2021; Emporium, 2024) for
training, which comprises 10.9 million images with detailed long-form captions (Liu et al., 2024a). Images are resized
and center-cropped to 256 √ó 256. We evaluate performance using GenEval (Ghosh et al., 2023) and DPG-Bench (Hu
et al., 2024). GenEval relies on object detection, making it highly sensitive to structure and texture‚Äîfactors closely tied
to human perceptual preference. If generated objects exhibit geometric inaccuracies or distorted textures, the detector
may fail to classify them correctly or produce duplicate detections. As a result, scores can be lower even when the
text‚Äìimage semantic alignment appears correct at a glance. Conversely, DPG-Bench employs a vision‚Äìlanguage model
as a judge, prioritizing high-level alignment over fine-grained details. This complementarity allows us to better interpret
trade-offs between structural fidelity and semantic alignment. We train with a batch size of approximately 730, a learning
rate of 10‚àí4, and apply EMA with a decay of 0.9999. Training for 200K iterations ensures convergence across various
generative feature spaces. For GenEval, we use the rewritten long-prompt version from Bagel (Deng et al., 2025),
consistent with our long-caption training data.
Variations in patch size and channel dimensionality along the sequence length alter the signal-to-noise ratio (SNR)
during interpolation between noise and latents. To maintain consistent SNR weighting across feature spaces, we apply
a shifted timestep t‚Ä≤ =
shift_factor¬∑t
1+(shift_factor‚àí1)¬∑t, where t is sampled from a Logit-Normal distribution (Esser et al., 2024;
Zheng et al., 2025). Since the sequence length is fixed across feature spaces, the shift factor depends only on the latent
8


Table 2 Comparison of reconstruction and generation performance.
The best results are shown in bold and the second-best
are underlined. Flux-VAE (stride 8) is listed for reference, and all other results correspond to the feature space with a stride 16.
Method
rFID (‚Üì)
PSNR (‚Üë)
LPIPS (‚Üì)
SSIM (‚Üë)
GenEval (‚Üë)
DPG-Bench (‚Üë)
Editing Reward (‚Üë)
Flux-VAE (Labs, 2024)
0.175
32.86
0.044
0.912
68.04
78.98
-0.271
MAR-VAE (Li et al., 2024)
0.534
26.18
0.135
0.715
75.75
83.19
0.056
VAVAE (Yao et al., 2025)
0.279
27.71
0.097
0.779
76.16
82.45
0.227
RAE (Zheng et al., 2025)
0.619
19.20
0.254
0.436
71.27
81.72
0.059
PS-VAE32c
0.584
24.53
0.168
0.662
76.22
84.25
0.274
PS-VAE96c
0.203
28.79
0.085
0.817
76.56
83.62
0.222
Figure 7 Coverage curves for generation and editing tasks across different feature spaces. By jointly providing rich
semantics and state-of-the-art reconstruction fidelity, PS-VAE consistently outperforms semantic-only RAE and pixel-only VAEs
across both generation and editing benchmarks. The strong semantic structure and well-regularized latent space of PS-VAE enable
significantly faster convergence during text-to-image training, while also facilitating better image understanding and, consequently,
stronger instruction-following in image editing. Meanwhile, higher reconstruction fidelity leads to more realistic structures and
textures in text-to-image generation, improved detail consistency between the edited and input images during editing, and thus better
overall performance in both tasks.
channel dimension Cvae and patch size Pvae: shift_factor =
q
CvaeP 2
vae
CbaseP 2
base , where Cbase = 16 and Pbase = 1. For instance,
Flux (Labs, 2024) (C = 16, P = 2) yields a shift factor of 2, while DINOv2-B (C = 768, P = 1) yields approximately
6.93. We conduct ablation studies to confirm that these calculated values are reasonable. For Flux, a factor of 2 yields a
better GenEval score compared to factors of 1 and 3, while for RAE, a factor of 6.93 performs best compared to 6 and 8.
We therefore apply this rule to all feature-space and channel-number ablation experiments. During inference, we use
50-step Euler sampling with a timestep shift of 3 and a classifier-free guidance scale of 6.5.
Instruction Editing. We utilize the OmniEdit dataset (Wei et al., 2024), which contains 1.2 million image‚Äìediting
pairs spanning seven editing categories: object replacement, object removal, object addition, attribute modification,
background swap, environment change, and style transfer. We resume the text-to-image checkpoint as the initialization
for the editing training. For evaluation, we adopt the corresponding editing subtasks from GEdit-Bench (Liu et al.,
2025) (Background Change, Color Alteration, Material Modification, Style Transfer, Subject Addition, Subject Removal,
Subject Replacement, and Tone Transformation), using their provided input images and prompts. Results are evaluated
using EditingReward (Wu et al., 2025), a state-of-the-art image editing scoring model that assesses both visual consistency
and instruction adherence. This setup ensures reliable and reproducible evaluation. Training and inference settings match
the text-to-image configuration, with models trained for 50k iterations and the best-performing checkpoints reported.
4.2
Reconstruction and Generation Performance of Different Feature Space
As shown in Table 2, our 96-channel PS-VAE96c achieves the highest reconstruction quality among all stride-16 VAEs,
trailing only Flux-VAE, which benefits from a finer stride of 8. In generation and editing tasks, both PS-VAE32c
and PS-VAE96c significantly outperform RAE with the same training budget. Specifically, PS-VAE32c achieves top
performance on DPG-Bench (Hu et al., 2024) and Editing Reward (Wu et al., 2025), ranking second on GenEval (Ghosh
et al., 2023). Meanwhile, PS-VAE96c leads on GenEval and ranks second and third on DPG-Bench and Editing Reward,
respectively, maintaining a clear advantage over the RAE baseline. Besides, benefiting from a well-constrained and
semantically rich latent space, PS-VAE converges faster than RAE and other VAEs (see Figure 7). Furthermore,
9


(b) Please change the background wall to a green forest with high mountains, bright sunlight, and distant flying birds
(c) Add more hair to the front, making it long and soft for a gentle look
MAR-VAE
VAVAE
Score:-1.69
(a) Remove the elderly man wearing glasses
Score:1.52
Score:-0.68
Score:-0.01
RAE
Ours
Input
Score:0.31
Score:0.48
Score:-1.09
Score:1.32
Score:-0.00
Score:-0.00
Score:-0.17
Score:0.21
Figure 8 Editing visual examples of models trained on different feature spaces. As shown in (a), both PS-VAE and
RAE exhibit reasonable visual grounding, correctly identifying the elderly man and the background wall in (a). However, RAE‚Äôs
performance is strongly limited by its weak reconstruction ability, resulting in inconsistent details with the input image (a,b,c). In
contrast, benefiting from strong semantic alignment and high-fidelity reconstruction, PS-VAE achieves accurate instruction following
while preserving consistent visual details between the input and edited images, such as the human face in (a,b,c).
enhanced detail fidelity enables PS-VAE to surpass standard VAE performance, highlighting the distinct advantage of the
pixel‚Äìsemantic constrained latent space.
Meanwhile, we observed in Table 2 and Figure 8 that VAEs trained solely on pixel reconstruction objectives (e.g.,
MAR-VAE and Flux-VAE) exhibit significantly lower prompt-following capabilities than models with semantically
structured latent spaces, e.g. PS-VAE32c, PS-VAE96c, and VAVAE. We hypothesize that instruction-based editing couples
two subtasks: semantic comprehension of the input latent and faithful generation based on the prompt. Consequently, a
semantically organized latent space facilitates source image interpretation and improves instruction adherence. This
aligns with findings from Bagel (Deng et al., 2025), where the injection of SigLIP2 (Tschannen et al., 2025) features
similarly enhances performance. We further verify this observation by the clear drop in editing performance when
the semantic reconstruction loss is removed from our training pipeline (denoted as P-VAE), decreasing from 0.22 for
PS-VAE to 0.04, as shown in Table 4. Besides, we also observed that RAE‚Äôs editing performance is constrained by
weak reconstruction capabilities, resulting in visual inconsistencies relative to the input, while PS-VAE effectively
balances semantic understanding with fine-grained detail preservation. That‚Äôs to say, PS-VAE retains the strong
instruction-following capability of RAE (see Figure 8.a) while achieving superior consistency in fine-grained regions
such as facial features (see Figure 8.a,b,c).
4.3
Scaling Behavior of Generative Models across PS-VAE Channel Dimensions
As shown in Figure 7 and Table 2, both PS-VAE32c and PS-VAE96c achieve state-of-the-art generation performance.
While PS-VAE96c provides better reconstruction quality, it slightly underperforms PS-VAE32c in generation metrics, likely
due to limited model capacity when modeling excessive fine-grained details. We further examine whether increasing
10


A panda holding a sign that 
reads ‚Äú‚ÄùMake REs Ready‚Äù ‚Ä¶
White-haired girl in flower-
filled car, dreamy analog 
film, colorful‚Ä¶
Cute jumping spider in 
pumpkin knit hat 
watching autumn leaf ‚Ä¶
Happy cartoon shark on beach 
wearing blue sneakers ‚Ä¶
Red rose bursting through 
cracked ice, blood drops...
Hyper-realistic Atlas holding 
Earth sphere labeled ‚ÄúPS-VAE‚Äù, 
dramatic, epic‚Ä¶
Late-90s indie portrait, pink-haired 
girl, golden-hour light, Ô¨Ålm grain‚Ä¶
Cute bird made of orange 
with green leaf wings
Macro chameleon camouÔ¨Çaged in 
vibrant leaves, eye 
staring camera‚Ä¶
Infernal knight versus divine 
queen on epic chessboard, 
elemental clash‚Ä¶
Low-poly dog warrior 
with sword, black gi, 
toy-like render‚Ä¶
D√≠a de los Muertos parade, 
Mexico City, ultra-realistic 
photography‚Ä¶
Figure 9 Text-to-image generation examples using PS-VAE96c.
Despite being trained only at 256 √ó 256 resolution, the
semantically structured and detail-preserving latent space enables the generator to accurately follow complex text prompts, yielding
images with correct structures, fine-grained textures, precise text rendering, realistic portraits, and flexible compositions of abstract
concepts. Prompts are simplified for visualization; full prompts can be found in the Supplementary Material.
Figure 10 Scaling behavior of 653M (dashed) and 1708M (solid) models under different PS-VAE channel sizes (32c/96c) on (a)
GenEval, (b) DPG-Bench, and (c) Editing Reward.
model capacity can mitigate this effect.
To assess whether the 96-channel variant offers a higher performance ceiling, we scale the generation backbone from
Qwen-0.5B to Qwen-1.5B (Bai et al., 2023) under a fixed training setting, guided by the scaling observations in (Esser
et al., 2024). This revealed distinct scaling behaviors as shown in Figure 10: PS-VAE96c exhibits consistent improvements
across all tasks, with GenEval rising from 76.56 to 78.14, DPG-Bench from 83.62 to 84.09, and Editing Reward
increasing significantly from 0.222 to 0.285. In contrast, PS-VAE32c demonstrates diminishing performance, showing
only marginal gains on GenEval (77.07 to 77.67) and slight degradation on both DPG-Bench (84.25 vs. 84.10) and
Editing Reward (0.274 vs. 0.228). These results indicate that higher-channel latent spaces possess superior scaling
properties and higher upper bounds when paired with a larger generative model. Investigating the correspondence
between high-channel latent spaces and large-scale generation backbones represents a promising direction for future
research.
Finally, we fine-tune Qwen-3B (Bai et al., 2023) with PS-VAE96c for 600k iterations, including 200k iterations under the
same training setting and an additional 400k iterations on a high-quality internal dataset. Benefiting from the strong
semantic alignment and high-fidelity reconstruction of our latent space, the generator produces images with accurate text
rendering, high-quality portraits, and flexible compositions of complex concepts (see Figure 9). Notably, it achieves
realistic textures and prompt-following capabilities while trained solely at 256 √ó 256 resolution. Extending to higher
resolutions will further amplify these capabilities, which we leave as a direction for future exploration.
11


Table 3 Comparison of reconstruction and generation performance between DINOv2 and SigLIP2.
Method
rFID (‚Üì)
PSNR (‚Üë)
LPIPS (‚Üì)
SSIM (‚Üë)
GenEval (‚Üë)
DPG-Bench (‚Üë)
Editing Reward (‚Üë)
PS-VAE96c(DINOv2-B)
0.203
28.79
0.085
0.817
76.56
83.62
0.222
PS-VAE96c(SigLIP2-so400m/14)
0.222
28.14
0.096
0.795
77.14
83.33
0.183
4.4
PS-VAE with SigLIP2: A Unified Encoder for Understanding and Generation
While SigLIP (Zhai et al., 2023; Tschannen et al., 2025) is widely adopted as a vision encoder for multimodal understanding,
we investigate whether our method enables SigLIP2 to function as a unified encoder for both understanding and generation.
Specifically, we utilize the SigLIP2-so400m/14 encoder from Bagel (Deng et al., 2025), training it under settings
identical to DINOv2 for fair comparison. The only modification is the weighting ratio between the semantic loss Ls and
the pixel reconstruction loss Lp, which is adjusted from 0.1:1 to 0.05:1. We observe that under equal loss weighting,
the reconstruction quality of SigLIP2 saturates earlier than DINOv2, likely due to its more high-level and abstract
representations.
Generation Performance. As shown in Table 3, PS-VAE96c(DINOv2) and PS-VAE96c(SigLIP2) achieve comparable
reconstruction quality with similar rFID, PSNR, LPIPS, and SSIM scores. In generation tasks, PS-VAE96c(SigLIP2)
attains a slightly higher GenEval score, while PS-VAE96c(DINOv2) performs better on DPG-Bench and Editing Reward.
Overall, both encoders demonstrate similar generation capabilities, confirming the robust transferability of our method
across different pretrained foundation models.
Understanding Performance. To evaluate whether optimizing for reconstruction degrades semantic representations,
we integrate our fine-tuned encoder into the original Bagel (Deng et al., 2025) pipeline, replacing the original encoder
while keeping the LLM parameters frozen. On standard benchmarks, we observe negligible performance degradation
in the zero-shot settings. For example, MME-P (Fu et al., 2023) scores decrease slightly from 1685 to 1652, and
VBench (Liu et al., 2024b) drops marginally from 85.0 to 84.7. These results validate that our proposed PS-VAE
preserves the core semantic capabilities of the original SigLIP2. It is worth noting that these results are obtained without
any fine-tuning of the LLM parameters. We hypothesize that jointly training with the LLM could further enable the
model to surpass the original baseline, which is left as a future extension, as the fine-tuned encoder preserves all visual
information while maintaining a well-structured semantic representation. Overall, these results suggest that SigLIP2,
optimized via PS-VAE, holds the potential as a unified encoder for future understanding and generation architectures.
5
Ablation Study
5.1
Evolution from RAE to PS-VAE
We conduct a comprehensive ablation study to analyze how reconstruction and generation performance evolve as we
progressively extend RAE to S-VAE and finally to PS-VAE. The results are summarized in Table 4, where MAR-VAE (Li
et al., 2024) is included as a performance reference.
From RAE to S-VAE: S-VAE is mainly trained with the semantic reconstruction loss LS and the KL loss LKL to
compact and regularize the feature space. Compared to RAE, S-VAE yields substantial improvements in generation
and editing performance (GenEval: 71.3‚Üí73.7, DPG-Bench: 81.7‚Üí83.6, Editing Reward: 0.06‚Üí0.12; see Table 4),
despite a marked degradation in reconstruction quality (PSNR: 19.2‚Üí17.78, SSIM: 0.436‚Üí0.390). This suggests that
RAE‚Äôs primary limitation stems not from reconstruction fidelity, but from the off-manifold nature of its high-dimensional
semantic features, which S-VAE effectively mitigates.
Besides, we also compare S-VAE with MAR-VAE (Li et al., 2024). Results indicate that while S-VAE trails MAR-VAE in
pixel-level reconstruction, its robust semantic representations allow it to outperform MAR-VAE on the semantics-oriented
DPG-Bench (Hu et al., 2024) and in instruction-based editing. However, S-VAE underperforms on GenEval (Ghosh et al.,
2023). Since GenEval relies on object detectors that are sensitive to texture and structure, the degraded pixel fidelity
of S-VAE prevents the generation model from learning realistic textures and fine-grained structural details, leading to
detection failures. This underscores the necessity of accurate reconstruction in VAEs for learning fine-grained object
details.
The Proposed PS-VAE: By integrating fine-grained detail supervision while maintaining semantic structure, our PS-VAE
12


Table 4 Evolving the Representation Autoencoder (RAE) into our Pixel‚ÄìSemantic Variational Autoencoder (PS-VAE).
Semantic regularization is essential for alleviating off-manifold behavior and improving generation and instruction-based editing
performance (RAE ‚ÜíS-VAE), while enriching pixel details without disrupting semantic structure yields PS-VAE with the best
overall performance. In contrast, a pixel-only VAE (P-VAE) trained with only pixel reconstruction loss exhibits degraded semantic
quality, as indicated by linear probing, along with inferior DPG-Bench and editing performance, despite achieving better pixel-level
reconstruction. This highlights the importance of semantic structure in the latent space.
Method
rFID (‚Üì)
PSNR (‚Üë)
LPIPS (‚Üì)
SSIM (‚Üë)
Linear Probe (‚Üë)
GenEval (‚Üë)
DPG (‚Üë)
Editing Reward (‚Üë)
MAR-VAE
0.534
26.18
0.135
0.715
5.4/15.1
75.7
83.2
0.06
RAE
0.619
19.20
0.254
0.436
83.0/96.6
71.3
81.7
0.06
S-VAE
1.407
17.78
0.296
0.390
81.1/95.7
73.7
83.6
0.12
P-VAE
0.398
29.81
0.073
0.850
11.8/26.7
75.2
82.1
0.04
PS-VAE
0.203
28.79
0.085
0.817
79.5/94.8
76.6
83.6
0.22
#C rFID (‚Üì) PSNR (‚Üë) LPIPS (‚Üì) SSIM (‚Üë)
32
0.584
24.53
0.168
0.662
48
0.475
25.43
0.147
0.697
64
0.423
26.65
0.124
0.744
80
0.292
27.38
0.107
0.772
96
0.203
28.79
0.085
0.817
112
0.159
30.51
0.064
0.865
256
0.156
30.30
0.065
0.860
Figure 11 Channel ablation of PS-VAE. Left: reconstruction metrics versus channel dimensionality. Middle/Right: convergence
curves on GenEval and DPG-Bench. As channel dimensionality increases, convergence becomes slightly slower; GenEval and
DPG-Bench remain stable from 32c to 96c, while DPG-Bench drops beyond 96c, suggesting an intrinsic latent dimensionality of
approximately ‚àº96 channels under our training setup. Further increasing the channel dimension primarily captures high-frequency
details, which may consume model capacity and interfere with semantic learning.
recovers high-frequency details without sacrificing semantic coherence. The 96-channel PS-VAE achieves state-of-the-art
reconstruction, surpassing MAR-VAE on GenEval (+0.9) while retaining the strong DPG-Bench performance of S-VAE.
In instruction-based editing, this enhanced visual fidelity significantly improves image‚Äìedit consistency, nearly doubling
the editing reward (0.12‚Üí0.22).
The Role of Semantic Structure (P-VAE): We isolate the specific contribution of the semantic structure of latent
space by training a Pixel-VAE (P-VAE) using solely the pixel reconstruction objective (LP ). Linear probing indicates
that its semantic quality regresses to the level of MAR-VAE. Consequently, DPG performance declines (83.6‚Üí82.6) and
editing reward drops sharply (0.22‚Üí0.04). This confirms that explicit semantic regularization within the latent space is
indispensable for text alignment and instruction following in both generation and editing tasks.
5.2
Ablation on Latent Channel of PS-VAE
To find the optimal latent space dimensionality of PS-VAE, we conduct a grid search (from 32 to 256) on the channel
number. As shown in Figure 11, reconstruction performance saturates at 112 channels, with further increases in
width yielding negligible gains. As for generation performance, increasing latent dimensionality slows convergence
(see Figure 11). However, final performance remains comparable between 32 and 96 channels on both GenEval and
DPG-Bench. A turning point occurs at 112 channels, where the DPG-Bench score drops noticeably by approximately
0.6 points. This observation suggests that, under our training setup, the intrinsic latent dimensionality required to
jointly preserve semantic structure and pixel-level fidelity is around ‚àº96 channels. Further increasing the channel
capacity mainly introduces high-frequency details that consume additional model capacity, which might hinder semantic
alignment, ultimately degrading generation performance.
5.3
Ablation on Encoder and Decoder Architectures
As shown in Table 5, projecting an unconstrained representation space into a compact, KL-regularized latent mani-
fold (Kingma and Welling, 2013) need non-trivial computational overhead. For example, a shallow 2-layer MLP is
insufficient to preserve rich semantic features within a 96-channel latent space, as evidenced by a substantial drop in
13


Table 5 Comparison of reconstruction and generation performance across different PS-VAE design variants. 2L-MLP
S-VAE denotes an S-VAE in which both the semantic encoder and the semantic decoder are implemented as 2-layer MLPs. DPixel on
DSemantic indicates that the pixel decoder is attached to the semantic decoder features during the final detail-enrichment training stage.
Method
rFID (‚Üì)
PSNR (‚Üë)
LPIPS (‚Üì)
SSIM (‚Üë)
Linear (‚Üë)
GenEval (‚Üë)
DPG-Bench (‚Üë)
PS-VAE
0.214
28.63
0.087
0.813
79.5 / 94.8
76.6
83.6
2L-MLP S-VAE
0.205
28.94
0.082
0.812
44.5 / 64.5
70.3
83.1
DPixel on DSemantic
0.193
29.64
0.077
0.840
80.4 / 95.4
74.4
83.6
linear probing accuracy. This suggests that limited mapping capacity prevents the semantic VAE from fully capturing the
intrinsic dimensionality of the presentation feature, leaving the effective intrinsic dimensionality below 96. Under the
KL regularization constraint, this further induces posterior collapse, which ultimately degrades generation performance.
We also explore an architecturally symmetric design that directly feeds the reconstructed semantic features into the pixel
decoder for final image reconstruction. While this approach improves reconstruction quality, we observe a degradation
in GenEval performance. We hypothesize that this stems from gradient interference, as both semantic and pixel
reconstruction objectives are backpropagated through a shared Transformer pathway. Alternatively, this architecture may
require a distinct re-balancing of loss weights to function effectively. We leave a deeper investigation of these dynamics
to future work.
5.4
Directly Enriching High-Dimensional Features Fails
Table 6 High-dimensional enrichment causes shortcut
reconstruction. RAE-HD greatly improves reconstruction
metrics but harms generation, indicating loss of semantic
structure and using shortcut reconstruction.
Method
rFID‚Üì
PSNR‚Üë
LPIPS‚Üì
SSIM‚Üë
GenEval‚Üë
RAE
0.619
19.20
0.254
0.436
71.3
RAE-HD
0.193
33.10
0.048
0.916
60.2
An alternative strategy to improve pixel reconstruction in-
volves training the pixel decoder directly on the original
high-dimensional feature space, while maintaining the se-
mantic reconstruction loss with a frozen DINOv2 encoder.
As shown in Table 6, while this approach yields rapid im-
provements in reconstruction quality, it leads to a significant
degradation in generation performance. The generated images
exhibit severe structural artifacts and incoherent textures(as
shown in Figure 12.a).
(a) Severe structural
artifacts and incoherent textures
(b) Reconstruction shortcut behavior: comparable 
reconstruction with only 32 of 768 channels.
768-Dec
32-Dec
Figure 12 Directly enriching details in a high-dimensional
space leads to severe generation artifacts (a). We further
verify that this behavior arises from reconstruction shortcuts
in high-dimensional feature spaces (b).
We attribute this failure to the inherent difficulty of constrain-
ing a high-dimensional latent space. Even with semantic-
preserving losses, the model can exploit shortcut solutions
by relying on a sparse subset of channels for reconstruction,
without inducing meaningful changes in feature distances
in high-dimensional spaces, where distance metrics tend to
become less informative. We verify this shortcut behavior
by showing that retraining a pixel decoder using only the 32
selected channels (out of 768) with the largest deviations be-
tween the fine-tuned encoder and the frozen DINOv2 features
is sufficient to achieve strong reconstruction performance (as
shown in Figure 12.b). This indicates that constraining detail enrichment to a compact, semantically regularized latent
space is essential, thereby validating the core design of our PS-VAE.
5.5
Conclusions
In this work, we show that powerful representation encoders, despite their strong discriminative ability, are not directly
suitable as generative spaces due to unconstrained feature distributions and insufficient image reconstruction fidelity.
Through systematic analysis, we identify off-manifold generation and poor reconstruction as the two key bottlenecks
limiting their performance in text-to-image generation and instruction-based editing. To address this, we propose a
Pixel‚ÄìSemantic VAE (PS-VAE) that maps representation features and pixel details into a compact, KL-regularized latent
space by properly finetuning pre-trained representation encoders under both pixel and semantic reconstruction objectives.
As a result, PS-VAE achieves state-of-the-art performance in reconstruction, generation, and image editing. We believe
this work offers a practical pathway toward unifying visual understanding and generation within a single encoder.
14


References
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen
technical report. arXiv preprint arXiv:2309.16609, 2023.
Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran,
Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Doll√°r, and Christoph
Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. arXiv, 2025.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties
in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
pages 3558‚Äì3568, 2021.
Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning
visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual
representations. arXiv preprint arXiv:2002.05709, 2020.
Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song,
Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025.
Caption Emporium.
conceptual-captions-cc12m-llavanext.
https://huggingface.co/datasets/CaptionEmporium/
conceptual-captions-cc12m-llavanext, 2024.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel
Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international
conference on machine learning, 2024.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al.
Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.
Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image
alignment. Advances in Neural Information Processing Systems, 36:52132‚Äì52152, 2023.
Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R√©mi Munos, and
Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning, 2020.
Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as a dialect:
Unifying visual understanding and generation via text-aligned representations. arXiv preprint arXiv:2506.18898, 2025.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation
learning. arXiv preprint arXiv:1911.05722, 2019.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders are scalable vision
learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000‚Äì16009, 2022.
Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic
alignment. arXiv preprint arXiv:2403.05135, 2024.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for
improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025.
Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024.
Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization.
Advances in Neural Information Processing Systems, 37:56424‚Äì56445, 2024.
Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, and Ying Shan. Toklip: Marry
visual tokens to clip for multimodal comprehension and generation. arXiv preprint arXiv:2505.05422, 2025.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing
systems, 36:34892‚Äì34916, 2023.
15


Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr,
and world knowledge, January 2024a. https://llava-vl.github.io/blog/2024-01-30-llava-next/.
Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han,
et al. Step1x-edit: A practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu,
et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216‚Äì233.
Springer, 2024b.
Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, and Yinfei Yang. Atoken: A
unified tokenizer for vision. arXiv preprint arXiv:2509.14476, 2025.
Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: A unified tokenizer
for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025.
Maxime Oquab, Timoth√©e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,
Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba,
Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand
Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language
supervision, 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684‚Äì10695,
2022.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):
211‚Äì252, 2015.
Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent
diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025.
Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting
pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024.
Oriane Sim√©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec,
Seungeun Yi, Micha√´l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timoth√©e Darcet, Th√©o
Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herv√©
J√©gou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025.
Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin.
Improving the diffusability of autoencoders. arXiv preprint arXiv:2502.14831, 2025.
Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu.
Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324,
2025.
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng
Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: A fully open,
vision-centric exploration of multimodal llms. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak,
and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 87310‚Äì87356. Curran Asso-
ciates, Inc., 2024. doi: 10.52202/079017-2771. https://proceedings.neurips.cc/paper_files/paper/2024/file/
9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf.
Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan
Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H√©naff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2:
Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025.
Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741,
2025.
Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist
models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024.
16


Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, and Wenhu Chen. Editreward: A human-aligned reward model for
instruction-guided image editing. arXiv preprint arXiv:2509.26346, 2025.
Wanghan Xu, Xiaoyu Yue, Zidong Wang, Yao Teng, Wenlong Zhang, Xihui Liu, Luping Zhou, Wanli Ouyang, and Lei Bai. Exploring
representation-aligned latent space for better generation. arXiv preprint arXiv:2502.00359, 2025.
Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025.
Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, KunPeng Du, Yi Wang, Limin
Wang, et al. Uniflow: A unified pixel flow tokenizer for visual understanding and generation. arXiv preprint arXiv:2510.10575,
2025.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023.
Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv
preprint arXiv:2510.11690, 2025.
Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke
Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth
International Conference on Learning Representations, 2025. https://openreview.net/forum?id=SI2hI0frk6.
17
